{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varroa Counter\n",
    "## 1.  Définition du problème\n",
    "\n",
    "Les colonies d'abeilles du monde entier sont infestées par un parasite qui s'appelle le varroa destructor.\n",
    "Ce parasite au nom barbare se fixe sur le corps des abeilles adultes et se nourrit de l'hémolymphe. Les femelles pénètrent aussi dans les cellules operculées pour se reproduire sur les larves, ce qui crée plusieurs générations au sein d'une même cellule.\n",
    "Le varroa transmet des virus aux abeilles et affaiblit leur système immunitaire.\n",
    "Si rien n'est entrepris pour stopper leur prolifération, les colonies finissent par s'effondrer durant l'automne ou l'hiver.\n",
    "\n",
    "\n",
    "<img src=\"varroa_destructor.jpg\" width=\"50%\">\n",
    "\n",
    "Une fois la récolte du miel effectuée, les apiculteurs effectuent différents traitements sur les colonies, notamment en utilisant de l'acide formique et de l'acide oxalique.\n",
    "Une fois le traitement effectué, l'apiculteur dépose une planchette sous la ruche afin d'évaluer le degré d'infestation des colonies. Quelques jours après le traitement, les varroas morts tombent sur le fond de la ruche. \n",
    "Les varroas ayant une taille de 1 à 2 mm, il devient très difficile de les compter lorsqu'ils sont nombreux. De plus, des résidus de cires d'abeilles tombent également des cadres, ce qui complique encore plus le comptage.\n",
    "\n",
    "<img src=\"fond_varroas.jpg\" width=\"50%\">\n",
    "\n",
    "Les varroas sont les petites formes sombres allongées et arrondies.\n",
    "\n",
    "L'objectif de ce projet est d'estimer automatiquement le niveau d'infestation par les varroas à partir d'une image. Il ne s'agit pas d'obtenir un décompte exact, notamment lorsque les varroas sont peu nombreux, mais plutôt de fournir un ordre de grandeur fiable en cas d'infestation importante — par exemple, distinguer une image contenant 50 varroas d'une autre en contenant 200. Ce type d'évaluation, difficile et fastidieux à réaliser manuellement, est ainsi automatisé pour faciliter le travail de l'apiculteur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collecte de données\n",
    "En recherchant des sets de données, j'en ai trouvé plusieurs disponibles sur https://universe.roboflow.com/, mais aucun dataset ne correspondait parfaitement à mes besoins:\n",
    "* Images de varroas sur les abeilles et non sur la planche\n",
    "* Images d'entraînement trop petites\n",
    "* Images avec des varroas labellisés mais qui ne ressemblent pas vraiment à des varroas\n",
    "\n",
    "J'ai donc créé un dataset avec mes propres images et j'en ai rajouté quelques-une trouvé sur le net. J'ai uploader le dataset sur roboflow sous le projet suivant: https://app.roboflow.com/varroa-counter/varroa-counter-v3/browse\n",
    "\n",
    "### Inspection des données\n",
    "Le dataset de données comprend des\n",
    "* 32 images d'entraînement (71%)\n",
    "* 13 images de validation (19%)\n",
    "* 7 images de test (12%)\n",
    "\n",
    "## 3. Préparation des Données\n",
    "La taille maximal pour le redimmensionnent étant de 2048 sur Roboflow, j'ai redimensionné les images 1536 x 2048 pixels. \n",
    "\n",
    "J'ai labellisé les 32 images en identifiant plus 1000 varroas. J'ai effectué ce travail très chronophage directement sur roboflow.\n",
    "\n",
    "\n",
    "Chaque image de ce set a donc maintenant un label associé. Un seul nom de classe est utilisé pour ce dataset: **varroa**\n",
    "Les labels associés à une image sont sauvegardés au format YOLO et comprennent simplement une suite de nombres comme ceci:\n",
    "* 0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
    "* 0 0.021484375 0.27880859375 0.009765625 0.0166015625\n",
    "* 0 0.0751953125 0.3349609375 0.009765625 0.015625\n",
    "* ...\n",
    "\n",
    "#### Explication du format YOLO\n",
    "\n",
    "```\n",
    "0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
    "│      │              │            │            │\n",
    "│      │              │            │            └── height (hauteur normalisée de la bounding box)\n",
    "│      │              │            └── width (largeur normalisée de la bounding box)\n",
    "│      │              └── y_center (position Y du centre, normalisée)\n",
    "│      └── x_center (position X du centre, normalisée)\n",
    "└── class_id (identifiant de la classe = 0 = varroa)\n",
    "```\n",
    "\n",
    "| Valeur | Signification | Exemple |\n",
    "|--------|---------------|---------|\n",
    "| `0` | ID de la classe | varroa (seule classe du dataset) |\n",
    "| `0.0259` | x_center | Le centre est à 2.6% de la largeur de l'image (très à gauche) |\n",
    "| `0.2544` | y_center | Le centre est à 25.4% de la hauteur de l'image |\n",
    "| `0.0107` | width | La box fait 1.07% de la largeur de l'image |\n",
    "| `0.0166` | height | La box fait 1.66% de la hauteur de l'image |\n",
    "\n",
    "## 4. Analyse Exploratoire des Données (AED)\n",
    "Les données contiennent des images avec des fonds de différentes couleurs et matières.\n",
    "\n",
    "## 5. Feature Engineering\n",
    "Le modèle devra faire de la détection d'objets. Un des challenges sera de détecter des objets très petits dans les images.\n",
    "Les 2 features importantes de la détection d'objets seront:\n",
    "* Classification d'image: déterminer si des varroas sont présents dans l'image\n",
    "* Localisation d'objet: trouver la position des varroas à l'aide de _bounding boxes_\n",
    "\n",
    "## 6. Modélisation\n",
    "Je choisi la classification comme modèle d'apprentissage.\n",
    "\n",
    "Je divise mon ensemble de donnée comme ceci:\n",
    "* 26 images d'entraînement (80%)\n",
    "* 4 images de validation (12%)\n",
    "* 2 images de test (8%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrmr4MPs9p7F"
      },
      "source": [
        "# Varroa Counter\n",
        "## 1.  Définition du problème\n",
        "\n",
        "Les colonies d'abeilles du monde entier sont infestées par un parasite qui s'appelle le varroa destructor.\n",
        "Ce parasite au nom barbare se fixe sur le corps des abeilles adultes et se nourrit de l'hémolymphe. Les femelles pénètrent aussi dans les cellules operculées pour se reproduire sur les larves, ce qui crée plusieurs générations au sein d'une même cellule.\n",
        "Le varroa transmet des virus aux abeilles et affaiblit leur système immunitaire.\n",
        "Si rien n'est entrepris pour stopper leur prolifération, les colonies finissent par s'effondrer durant l'automne ou l'hiver.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/geda/varroacounter/blob/main/varroa_destructor.jpg?raw=1\" width=\"50%\">\n",
        "\n",
        "Une fois la récolte du miel effectuée, les apiculteurs effectuent différents traitements sur les colonies, notamment en utilisant de l'acide formique et de l'acide oxalique.\n",
        "Une fois le traitement effectué, l'apiculteur dépose une planchette sous la ruche afin d'évaluer le degré d'infestation des colonies. Quelques jours après le traitement, les varroas morts tombent sur le fond de la ruche.\n",
        "Les varroas ayant une taille de 1 à 2 mm, il devient très difficile de les compter lorsqu'ils sont nombreux. De plus, des résidus de cires d'abeilles tombent également des cadres, ce qui complique encore plus le comptage.\n",
        "\n",
        "<img src=\"https://github.com/geda/varroacounter/blob/main/fond_varroas.jpg?raw=1\" width=\"50%\">\n",
        "\n",
        "Les varroas sont les petites formes sombres allongées et arrondies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehRhkqZa9p7I"
      },
      "source": [
        "## 2. Collecte de données\n",
        "En recherchant des sets de données, j'en ai trouvé plusieurs disponibles sur https://universe.roboflow.com/, mais aucun dataset ne correspondait parfaitement à mes besoins:\n",
        "* Images de varroas sur les abeilles et non sur la planche\n",
        "* Images d'entraînement trop petites\n",
        "* Images avec des varroas labellisés mais qui ne ressemblent pas vraiment à des varroas\n",
        "\n",
        "J'ai donc créé un dataset avec mes propres images et j'en ai rajouté quelques-une trouvé sur le net. J'ai uploader le dataset sur roboflow sous le projet suivant: https://app.roboflow.com/varroa-counter/varroa-counter-v3/browse\n",
        "\n",
        "### Inspection des données\n",
        "Le dataset de données comprend des\n",
        "* 32 images d'entraînement (71%)\n",
        "* 13 images de validation (19%)\n",
        "* 7 images de test (12%)\n",
        "\n",
        "## 3. Préparation des Données\n",
        "La taille maximal pour le redimmensionnent étant de 2048 sur Roboflow, j'ai redimensionné les images 1536 x 2048 pixels.\n",
        "\n",
        "J'ai labellisé les 32 images en identifiant plus 1000 varroas. J'ai effectué ce travail très chronophage directement sur roboflow.\n",
        "\n",
        "\n",
        "Chaque image de ce set a donc maintenant un label associé. Un seul nom de classe est utilisé pour ce dataset: **varroa**\n",
        "Les labels associés à une image sont sauvegardés au format YOLO et comprennent simplement une suite de nombres comme ceci:\n",
        "* 0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
        "* 0 0.021484375 0.27880859375 0.009765625 0.0166015625\n",
        "* 0 0.0751953125 0.3349609375 0.009765625 0.015625\n",
        "* ...\n",
        "\n",
        "#### Explication du format YOLO\n",
        "\n",
        "```\n",
        "0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
        "│      │              │            │            │\n",
        "│      │              │            │            └── height (hauteur normalisée de la bounding box)\n",
        "│      │              │            └── width (largeur normalisée de la bounding box)\n",
        "│      │              └── y_center (position Y du centre, normalisée)\n",
        "│      └── x_center (position X du centre, normalisée)\n",
        "└── class_id (identifiant de la classe = 0 = varroa)\n",
        "```\n",
        "\n",
        "| Valeur | Signification | Exemple |\n",
        "|--------|---------------|---------|\n",
        "| `0` | ID de la classe | varroa (seule classe du dataset) |\n",
        "| `0.0259` | x_center | Le centre est à 2.6% de la largeur de l'image (très à gauche) |\n",
        "| `0.2544` | y_center | Le centre est à 25.4% de la hauteur de l'image |\n",
        "| `0.0107` | width | La box fait 1.07% de la largeur de l'image |\n",
        "| `0.0166` | height | La box fait 1.66% de la hauteur de l'image |\n",
        "\n",
        "## 4. Analyse Exploratoire des Données (AED)\n",
        "Les données contiennent des images avec des fonds de différentes couleurs et matières.\n",
        "\n",
        "## 5. Feature Engineering\n",
        "Le modèle devra faire de la détection d'objets. Un des challenges sera de détecter des objets très petits dans les images.\n",
        "Les 2 features importantes de la détection d'objets seront:\n",
        "* Classification d'image: déterminer si des varroas sont présents dans l'image\n",
        "* Localisation d'objet: trouver la position des varroas à l'aide de _bounding boxes_\n",
        "\n",
        "## 6. Modélisation\n",
        "Je choisi la classification comme modèle d'apprentissage.\n",
        "\n",
        "Je divise mon ensemble de donnée comme ceci:\n",
        "* 26 images d'entraînement (80%)\n",
        "* 4 images de validation (12%)\n",
        "* 2 images de test (8%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 984
        },
        "id": "84rjtpAA9p7J",
        "outputId": "15ff9152-2410-4091-91b0-e68be58d911f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.12/dist-packages (1.2.13)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2026.1.4)\n",
            "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.7)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.10.0.84)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.5.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "{\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"None\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1024467227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroboflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoboflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROBOFLOW_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"varroa-counter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"varroa-counter-large\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, model_format, notebook)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monboarding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36mauth\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"onboarding\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/roboflow/__init__.py\u001b[0m in \u001b[0;36mcheck_key\u001b[0;34m(api_key, model, notebook, num_retries)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: {\n    \"error\": {\n        \"message\": \"This API key does not exist (or has been revoked).\",\n        \"status\": 401,\n        \"type\": \"OAuthException\",\n        \"hint\": \"You may retrieve your API key via the Roboflow Dashboard. Go to Account > Roboflow Keys to retrieve yours.\",\n        \"key\": \"None\"\n    }\n}"
          ]
        }
      ],
      "source": [
        "from requests import api\n",
        "!pip install roboflow\n",
        "\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "from google.colab import userdata\n",
        "# api_key = str(os.getenv(\"ROBOFLOW_API_KEY\"))\n",
        "api_key = userdata.get(\"ROBOFLOW_API_KEY\")\n",
        "rf = Roboflow(api_key=api_key)\n",
        "\n",
        "\n",
        "\n",
        "project = rf.workspace(\"varroa-counter\").project(\"varroa-counter-large\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# entrainement du modèle \n",
    "# Je créer un nouveau modèle depuis zéro\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "# lancement de l'entraînement avec seulement 1 seul passage et en abaissant le nombre de batch (nombre d'images traitées simultanément).\n",
    "# le but est de tester si les capacitées de ma machine sont suffisantes pour entraîner mon modèle\n",
    "results = model.train(data=\"varroa-counter-v3-2/data.yaml\", epochs=1, imgsz=2048, batch=8,\n",
    "                      max_det=2000, conf=0.1, iou = 0.5)\n",
    "print (results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impossible d'entraîner mon modèle sur ma machine, car elle n'a pas de GPU et le kernel crash. \n",
    "J'ai fait un essai en utilisant google colab, qui permet gratuitement d'entraîner des modèles avec un peu de GPU. Malheureusement j'ai le même problème sur google colab qui m'indique que ma session a planté après avoir utilisé toute la RAM disponible.\n",
    "\n",
    "Le problème est l'entraînement de modèle avec des images de grandes tailles car cela nécessite beaucoup de mémoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changement de stratégie: recherche d'un modèle existant\n",
    "Je décide donc de rechercher un modèle existant pouvant couvrir mes besoins et j'ai trouvé le travail de recherche suivant: https://www.mdpi.com/2077-0472/15/9/969\n",
    "\n",
    "**Référence:**\n",
    "> Yániz, J.; Casalongue, M.; Martinez-de-Pison, F.J.; Silvestre, M.A.; Consortium, B.; Santolaria, P.; Divasón, J. *An AI-Based Open-Source Software for Varroa Mite Fall Analysis in Honeybee Colonies*. Agriculture 2025, 15, 969. https://doi.org/10.3390/agriculture15090969\n",
    "\n",
    "Leurs modèle a été entraîné avec un dataset de 357 images sur plus de 500 epochs. Ils ont également livré un programme écrit en python permettant d'upload ses images et d'effectuer la détection des varroas avec leurs modèle.\n",
    "Le code source est disponible sous github ainsi que leurs modèle entraîné: https://github.com/jodivaso/varrodetector/blob/main/model/weights/best.pt\n",
    "\n",
    "En analysant le code j'ai trouvé particulièrement intéressant qu'ils utilisent une taille d'image assez grande de 6016 pixels (https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507C42-L2507C46).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation du modèle\n",
    "Je décide de rebalancer toutes les images de mon dataset en set de test et de n'appliquer aucun redimensionnement d'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"tEQkmVJiCxGOZMuDLR6d\")\n",
    "project = rf.workspace(\"varroa-counter\").project(\"varroa-counter-v3\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et de lancer le test du modèle avec mon dataset de test comprenant 32 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.30.2 ms, read: 71.612.7 MB/s, size: 1681.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3\\test\\labels.cache... 32 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 32/32 15.9Kit/s 0.0s\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 209, len(boxes) = 1057. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "# Effectuer la détection sur l'image\n",
    "results = model.val(\n",
    "    data=\"varroa-counter-v3-3/data.yaml\",\n",
    "    split='test',  # or 'val' for validation set\n",
    "    imgsz=6016, # même valeur celle utilisée dans le code https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507\n",
    "    batch=16,    \n",
    "    conf=0.1,  # Seuil de confiance\n",
    "    iou=0.5,\n",
    "    max_det=2000,\n",
    "    save_json=True,\n",
    "    save=True,\n",
    "    show_labels=False,\n",
    "    show_conf=False,\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "# Print results\n",
    "print(f\"Precision: {results.box.p}\")\n",
    "print(f\"Recall: {results.box.r}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP50-95: {results.box.map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En résumé la qualité de ce modèle par rapport à mon problème est faible*\n",
    "\n",
    "**Precision (0.42)** — Sur toutes les détections faites, seulement 42% sont correctes. Presque 1 détection sur 2 est un faux positif (détecte un varroa là où il n'y en a pas).\n",
    "\n",
    "**Recall (0.37)** — Le modèle ne détecte que 37% des varroas réellement présents. Il en rate presque 2 sur 3.\n",
    "\n",
    "**mAP50 (0.33)** — La performance globale à un seuil IoU de 50% est basse. Un modèle correct vise >0.5, un bon modèle >0.7.\n",
    "\n",
    "**mAP50-95 (0.10)** — La performance moyennée sur des seuils IoU stricts est très faible. Le modèle localise mal les objets même quand il les trouve.\n",
    "\n",
    "Je décide donc de faire un test avec 1 seule image afin de déterminer plus précisément où le problème se trouve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "'dataset.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33mmodel_mdpi_3291496/weights/best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Effectuer la détection sur l'image\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvarroa-counter-v3-3/test/images/IMG_0223_jpg.rf.a0db71f36b6b62fd38a1deca87b0a1f0.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6016\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_conf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mline_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVarroas détectés: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results[\u001b[32m0\u001b[39m].boxes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:611\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    610\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:175\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    172\u001b[39m     LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args.batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input of shape (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.args.batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, 3, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimgsz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.args.data).rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task == \u001b[33m\"\u001b[39m\u001b[33mclassify\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = check_cls_dataset(\u001b[38;5;28mself\u001b[39m.args.data, split=\u001b[38;5;28mself\u001b[39m.args.split)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\utils.py:400\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_det_dataset\u001b[39m(dataset: \u001b[38;5;28mstr\u001b[39m, autodownload: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    387\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Download, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[32m    388\u001b[39m \n\u001b[32m    389\u001b[39m \u001b[33;03m    This function checks the availability of a specified dataset, and if not found, it has the option to download and\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m \u001b[33;03m        (dict[str, Any]): Parsed dataset information and paths.\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     file = \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n\u001b[32m    403\u001b[39m     extract_dir = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\checks.py:636\u001b[39m, in \u001b[36mcheck_file\u001b[39m\u001b[34m(file, suffix, download, download_dir, hard)\u001b[39m\n\u001b[32m    634\u001b[39m files = glob.glob(\u001b[38;5;28mstr\u001b[39m(ROOT / \u001b[33m\"\u001b[39m\u001b[33m**\u001b[39m\u001b[33m\"\u001b[39m / file), recursive=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m glob.glob(\u001b[38;5;28mstr\u001b[39m(ROOT.parent / file))  \u001b[38;5;66;03m# find file\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMultiple files match \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, specify exact path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiles\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: 'dataset.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "# Effectuer la détection sur l'image\n",
    "results = model.predict(\n",
    "    source=\"varroa-counter-v3-3/test/images/IMG_0223_jpg.rf.a0db71f36b6b62fd38a1deca87b0a1f0.jpg\",\n",
    "    imgsz=6016,\n",
    "    max_det=2000,\n",
    "    conf=0.1,\n",
    "    iou=0.5,\n",
    "    save=True,\n",
    "    show_labels=False,\n",
    "    show_conf=False,\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "print(f\"Varroas détectés: {len(results[0].boxes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un zoom sur l'image générée par la détection\n",
    "![image annoté après prédiction](IMG_0223_jpg.rf.a0db71f36b6b62fd38a1deca87b0a1f0.jpg)\n",
    "On constate les problèmes suivants:\n",
    "- le modèle confond les gouttes d'eau et les varroas\n",
    "- le modèle oublie des varroas pourtant bien visibles\n",
    "\n",
    "Je décide de faire une prédiction valider le modèle avec uniquement les images provenant du site espagnol. Je ne sais pas si elles ont été utilisées lors de l'entraînement du modèle, mais décide de faire une comparaison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset filtré créé dans: varroa-counter-v3-3-IMG6\n",
      "Images copiées: 5\n",
      "  test: 5 images\n",
      "  valid: 0 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_filtered_dataset(source_dir, dest_dir, prefix=\"IMG_6\"):\n",
    "    \"\"\"\n",
    "    Crée une copie d'un dataset YOLO en ne gardant que les images\n",
    "    dont le nom commence par le préfixe spécifié.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Chemin du dataset source (ex: \"varroa-counter-v3-3\")\n",
    "        dest_dir: Chemin du dataset destination\n",
    "        prefix: Préfixe des images à conserver (ex: \"IMG_6\")\n",
    "    \"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    dest_dir = Path(dest_dir)\n",
    "\n",
    "    # Copier data.yaml et fichiers README\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in source_dir.glob(\"*.yaml\"):\n",
    "        shutil.copy2(f, dest_dir / f.name)\n",
    "    for f in source_dir.glob(\"*.txt\"):\n",
    "        shutil.copy2(f, dest_dir / f.name)\n",
    "\n",
    "    total_copied = 0\n",
    "\n",
    "    # Parcourir les splits (train, valid, test)\n",
    "    for split_dir in source_dir.iterdir():\n",
    "        if not split_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        images_dir = split_dir / \"images\"\n",
    "        labels_dir = split_dir / \"labels\"\n",
    "\n",
    "        if not images_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Créer les dossiers de destination\n",
    "        dest_images = dest_dir / split_dir.name / \"images\"\n",
    "        dest_labels = dest_dir / split_dir.name / \"labels\"\n",
    "        dest_images.mkdir(parents=True, exist_ok=True)\n",
    "        dest_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Copier les images correspondant au préfixe et leurs labels\n",
    "        for img_file in images_dir.iterdir():\n",
    "            if img_file.name.startswith(prefix):\n",
    "                shutil.copy2(img_file, dest_images / img_file.name)\n",
    "\n",
    "                # Copier le label correspondant (.txt)\n",
    "                label_file = labels_dir / (img_file.stem + \".txt\")\n",
    "                if label_file.exists():\n",
    "                    shutil.copy2(label_file, dest_labels / label_file.name)\n",
    "\n",
    "                total_copied += 1\n",
    "\n",
    "    print(f\"Dataset filtré créé dans: {dest_dir}\")\n",
    "    print(f\"Images copiées: {total_copied}\")\n",
    "\n",
    "    # Lister le contenu\n",
    "    for split_dir in sorted(dest_dir.iterdir()):\n",
    "        if split_dir.is_dir():\n",
    "            imgs = list((split_dir / \"images\").glob(\"*\"))\n",
    "            print(f\"  {split_dir.name}: {len(imgs)} images\")\n",
    "\n",
    "\n",
    "# Créer le dataset filtré avec uniquement les images IMG_6xxx\n",
    "create_filtered_dataset(\n",
    "    source_dir=\"varroa-counter-v3-3\",\n",
    "    dest_dir=\"varroa-counter-v3-3-IMG6\",\n",
    "    prefix=\"IMG_6\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 85.96.4 MB/s, size: 1745.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3-IMG6\\test\\labels... 5 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5/5 118.5it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3-IMG6\\test\\labels.cache\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 209, len(boxes) = 745. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 1/1 123.3s/it 2:03\n",
      "                   all          5        745      0.609      0.483      0.483      0.148\n",
      "Speed: 890.8ms preprocess, 22679.2ms inference, 0.3ms loss, 49.0ms postprocess per image\n",
      "Saving D:\\dev\\runs\\detect\\val65\\predictions.json...\n",
      "Results saved to \u001b[1mD:\\dev\\runs\\detect\\val65\u001b[0m\n",
      "Precision: [    0.60893]\n",
      "Recall: [     0.4828]\n",
      "mAP50: 0.48298499787085747\n",
      "mAP50-95: 0.1482758188990101\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "# Effectuer la détection sur l'image\n",
    "results = model.val(\n",
    "    data=\"varroa-counter-v3-3-IMG6/data.yaml\",\n",
    "    split='test',  # or 'val' for validation set\n",
    "    imgsz=6016, # même valeur celle utilisée dans le code https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507\n",
    "    batch=16,    \n",
    "    conf=0.1,  # Seuil de confiance\n",
    "    iou=0.5,\n",
    "    max_det=2000,\n",
    "    save_json=True,\n",
    "    save=True,\n",
    "    show_labels=False,\n",
    "    show_conf=False,\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "# Afficher les résultats\n",
    "# Print results\n",
    "print(f\"Precision: {results.box.p}\")\n",
    "print(f\"Recall: {results.box.r}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces résultats montrent une performance améliorée par rapport à la validation du set complet, mais quand-même une performance relativement faible.\n",
    "\n",
    "Precision (0.61) — Sur toutes les détections faites, 61% sont correctes. Environ 4 détections sur 10 sont des faux positifs.\n",
    "\n",
    "Recall (0.48) — Le modèle ne détecte que 48% des varroas réellement présents. Il en rate plus de la moitié.\n",
    "\n",
    "mAP50 (0.48) — Performance globale à IoU 50% insuffisante. Un modèle correct vise >0.5, un bon modèle >0.7.\n",
    "\n",
    "mAP50-95 (0.15) — La localisation précise est très faible. Même quand le modèle trouve un varroa, la bounding box est souvent mal positionnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\dev\\ia\\cours\\varroacounter\\varroas\\Sample images\\IMG_6187.jpg: 6016x4512 85 varroas, 7622.3ms\n",
      "Speed: 525.4ms preprocess, 7622.3ms inference, 28.9ms postprocess per image at shape (1, 3, 6016, 4512)\n",
      "Results saved to \u001b[1mD:\\dev\\runs\\detect\\predict15\u001b[0m\n",
      "Varroas détectés: 85\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "# Effectuer la détection sur l'image\n",
    "results = model.predict(\n",
    "    source=\"varroas/Sample images/IMG_6187.jpg\",\n",
    "    imgsz=6016,\n",
    "    max_det=2000,\n",
    "    conf=0.1,\n",
    "    iou=0.5,\n",
    "    save=True,\n",
    "    show_labels=False,\n",
    "    show_conf=False,\n",
    "    line_width=2\n",
    ")\n",
    "\n",
    "print(f\"Varroas détectés: {len(results[0].boxes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate sur l'image résultant de la prédiction les problèmes suivants:\n",
    "- lorsque le varroa est posé sur de la cire, il est mal détecté\n",
    "- lorsque 2 varroas sont côte à côte, parfois un seul varroa est détecté\n",
    "\n",
    "![image de l'étude annoté](IMG_6187_predict_15_zoom.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sahi in d:\\dev\\python\\python312\\lib\\site-packages (0.11.36)\n",
      "Requirement already satisfied: click in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (8.3.1)\n",
      "Requirement already satisfied: fire in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (0.7.1)\n",
      "Requirement already satisfied: opencv-python<=4.11.0.86 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=8.2.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (12.0.0)\n",
      "Requirement already satisfied: pybboxes==0.1.6 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (0.1.6)\n",
      "Requirement already satisfied: pyyaml in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (6.0.3)\n",
      "Requirement already satisfied: requests in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.32.5)\n",
      "Requirement already satisfied: shapely>=2.0.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.1.2)\n",
      "Requirement already satisfied: terminaltables in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (3.1.10)\n",
      "Requirement already satisfied: torch>=2.4.1 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (4.67.1)\n",
      "Requirement already satisfied: numpy in d:\\dev\\python\\python312\\lib\\site-packages (from pybboxes==0.1.6->sahi) (2.2.6)\n",
      "Requirement already satisfied: filelock in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.4.1->sahi) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\david\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.48.2->sahi) (0.4.6)\n",
      "Requirement already satisfied: termcolor in d:\\dev\\python\\python312\\lib\\site-packages (from fire->sahi) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.4.1->sahi) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (2025.11.12)\n",
      "Performing prediction on 35 slices.\n"
     ]
    }
   ],
   "source": [
    "!pip install sahi\n",
    "\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "\n",
    "model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",  # Compatible YOLO11\n",
    "    model_path=\"model_mdpi_3291496/weights/best.pt\",\n",
    "    confidence_threshold=0.1\n",
    ")\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    image=\"varroa-counter-v3-3/test/images/IMG_0223_jpg.rf.a0db71f36b6b62fd38a1deca87b0a1f0.jpg\",\n",
    "    detection_model=model,\n",
    "    slice_height=1024,\n",
    "    slice_width=1024,\n",
    "    overlap_height_ratio=0.2,\n",
    "    overlap_width_ratio=0.2\n",
    ")\n",
    "\n",
    "result.export_visuals(export_dir=\"results/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "results = model.train(\n",
    "    data=\"varroa-counter-v3-2/data.yaml\",\n",
    "    epochs=150,\n",
    "    imgsz=2048,\n",
    "    batch=4,\n",
    "    freeze=10  # Gèle les premières couches, ajuste seulement les dernières\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 138.130.7 MB/s, size: 1597.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3\\test\\labels... 32 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 32/32 228.3it/s 0.1s.2s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3\\test\\labels.cache\n",
      "WARNING Box and segment counts should be equal, but got len(segments) = 209, len(boxes) = 1057. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 14.3s/it 28.6s49.4s\n",
      "                   all         32       1057      0.566      0.459       0.51      0.268\n",
      "Speed: 18.7ms preprocess, 592.8ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
      "Results saved to \u001b[1mD:\\dev\\runs\\detect\\val34\u001b[0m\n",
      "Precision: [    0.56554]\n",
      "Recall: [    0.45885]\n",
      "mAP50: 0.5097353357108658\n",
      "mAP50-95: 0.2684233878827464\n"
     ]
    }
   ],
   "source": [
    "# 2. Tester le modèle fine-tuné\n",
    "# Le meilleur modèle est automatiquement sauvegardé dans runs/detect/trainX/weights/best.pt\n",
    "model_finetuned = YOLO('runs/train23/weights/best.pt')\n",
    "\n",
    "# Validation sur le set de test\n",
    "results = model_finetuned.val(\n",
    "    data=\"varroa-counter-v3-3/data.yaml\",\n",
    "    split='test',  # ou 'val'\n",
    "    imgsz=2048,\n",
    "    conf=0.1,\n",
    "    iou=0.5,\n",
    "    max_det=2000,\n",
    "    save=True\n",
    ")\n",
    "\n",
    "print(f\"Precision: {results.box.p}\")\n",
    "print(f\"Recall: {results.box.r}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP50-95: {results.box.map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fkp0QrC9p7P",
        "outputId": "67638d95-8b43-4726-a98d-600794954648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in varroa-counter-v3-3 to yolov11:: 100%|██████████| 50577/50577 [00:02<00:00, 20475.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to varroa-counter-v3-3 in yolov11:: 100%|██████████| 70/70 [00:00<00:00, 555.15it/s]\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"tEQkmVJiCxGOZMuDLR6d\")\n",
        "project = rf.workspace(\"varroa-counter\").project(\"varroa-counter-v3\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "\u001b[34m\u001b[1mval: \u001b[0mError loading data from None\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:165\u001b[39m, in \u001b[36mBaseDataset.get_img_files\u001b[39m\u001b[34m(self, img_path)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m img_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img_path, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [img_path]:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     p = \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# os-agnostic\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.is_dir():  \u001b[38;5;66;03m# dir\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\pathlib.py:1162\u001b[39m, in \u001b[36mPath.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1161\u001b[39m     warnings._deprecated(\u001b[33m\"\u001b[39m\u001b[33mpathlib.PurePath(**kwargs)\u001b[39m\u001b[33m\"\u001b[39m, msg, remove=(\u001b[32m3\u001b[39m, \u001b[32m14\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\pathlib.py:373\u001b[39m, in \u001b[36mPurePath.__init__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    374\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33margument should be a str or an os.PathLike \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    375\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mobject where __fspath__ returns a str, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    376\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    377\u001b[39m paths.append(path)\n",
      "\u001b[31mTypeError\u001b[39m: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33mmodel_mdpi_3291496/weights/best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVarroa-board-1/data.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Utiliser le split 'test'\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m6016\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_txt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_conf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRésultats de validation sur Varroa-board-1:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrécision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.box.p\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:611\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    610\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:186\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.rect = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.stride = model.stride  \u001b[38;5;66;03m# used in get_dataloader() for padding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28mself\u001b[39m.dataloader = \u001b[38;5;28mself\u001b[39m.dataloader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m model.eval()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.compile:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:313\u001b[39m, in \u001b[36mDetectionValidator.get_dataloader\u001b[39m\u001b[34m(self, dataset_path, batch_size)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_path: \u001b[38;5;28mstr\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m) -> torch.utils.data.DataLoader:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct and return dataloader.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m \u001b[33;03m        (torch.utils.data.DataLoader): DataLoader for validation.\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m build_dataloader(\n\u001b[32m    315\u001b[39m         dataset,\n\u001b[32m    316\u001b[39m         batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    321\u001b[39m         pin_memory=\u001b[38;5;28mself\u001b[39m.training,\n\u001b[32m    322\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:301\u001b[39m, in \u001b[36mDetectionValidator.build_dataset\u001b[39m\u001b[34m(self, img_path, mode, batch)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m, batch: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.utils.data.Dataset:\n\u001b[32m    291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build YOLO Dataset.\u001b[39;00m\n\u001b[32m    292\u001b[39m \n\u001b[32m    293\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    299\u001b[39m \u001b[33;03m        (Dataset): YOLO dataset.\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_yolo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\build.py:236\u001b[39m, in \u001b[36mbuild_yolo_dataset\u001b[39m\u001b[34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[39;00m\n\u001b[32m    235\u001b[39m dataset = YOLOMultiModalDataset \u001b[38;5;28;01mif\u001b[39;00m multi_modal \u001b[38;5;28;01melse\u001b[39;00m YOLODataset\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\dataset.py:88\u001b[39m, in \u001b[36mYOLODataset.__init__\u001b[39m\u001b[34m(self, data, task, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.use_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_keypoints), \u001b[33m\"\u001b[39m\u001b[33mCan not use both segments and keypoints.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchannels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:117\u001b[39m, in \u001b[36mBaseDataset.__init__\u001b[39m\u001b[34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.channels = channels\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.cv2_flag = cv2.IMREAD_GRAYSCALE \u001b[38;5;28;01mif\u001b[39;00m channels == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cv2.IMREAD_COLOR\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28mself\u001b[39m.im_files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_img_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.labels = \u001b[38;5;28mself\u001b[39m.get_labels()\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.update_labels(include_class=classes)  \u001b[38;5;66;03m# single_cls and include_class\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:181\u001b[39m, in \u001b[36mBaseDataset.get_img_files\u001b[39m\u001b[34m(self, img_path)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m im_files, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFORMATS_HELP_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mError loading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mHELP_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fraction < \u001b[32m1\u001b[39m:\n\u001b[32m    183\u001b[39m     im_files = im_files[: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(im_files) * \u001b[38;5;28mself\u001b[39m.fraction)]  \u001b[38;5;66;03m# retain a fraction of the dataset\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: \u001b[34m\u001b[1mval: \u001b[0mError loading data from None\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance."
     ]
    }
   ],
   "source": [
    "# Test du modèle avec le dataset Varroa-board-1\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "results = model.val(\n",
    "    data=\"Varroa-board-1/data.yaml\",\n",
    "    split='test',  # Utiliser le split 'test'\n",
    "    imgsz=(6016), max_det=2000, conf=0.1, iou = 0.5,\n",
    "    save=True, show_labels=False, line_width=2, save_txt=True, save_conf=True\n",
    ")\n",
    "\n",
    "print(\"\\nRésultats de validation sur Varroa-board-1:\")\n",
    "print(f\"Précision: {results.box.p}\")\n",
    "print(f\"Recall: {results.box.r}\")\n",
    "print(f\"mAP50: {results.box.map50}\")\n",
    "print(f\"mAP50-95: {results.box.map}\")\n",
    "\n",
    "#Résultats de validation sur Varroa-board-1:\n",
    "#Précision: [    0.80237]\n",
    "#Recall: [    0.42177]\n",
    "#mAP50: 0.584942355545071\n",
    "#mAP50-95: 0.2293648037166341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de SAHI pour la détection par découpage\n",
    "!pip install sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 6 slices.\n",
      "Image exportée dans: results/detection_result.png\n",
      "\n",
      "======================================================================\n",
      "RÉSUMÉ DE LA DÉTECTION\n",
      "======================================================================\n",
      "\n",
      "Nombre total de varroas détectés: 245\n",
      "\n",
      "Statistiques de confiance:\n",
      "  - Confiance moyenne: 0.629\n",
      "  - Confiance min: 0.101\n",
      "  - Confiance max: 0.939\n",
      "\n",
      "Distribution par niveau de confiance:\n",
      "  - 0.1 - 0.3: 61 détections\n",
      "  - 0.3 - 0.5: 26 détections\n",
      "  - 0.5 - 0.7: 15 détections\n",
      "  - 0.7 - 0.9: 98 détections\n",
      "  - 0.9 - 1.0: 45 détections\n",
      "\n",
      "Statistiques des bounding boxes (en pixels):\n",
      "  - Aire moyenne: 3428.8 px²\n",
      "  - Aire min: 294.6 px²\n",
      "  - Aire max: 8172.9 px²\n",
      "\n",
      "======================================================================\n",
      "DÉTAILS DES 10 PREMIÈRES DÉTECTIONS\n",
      "======================================================================\n",
      "\n",
      "Détection #1:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.939\n",
      "  - Bounding box: x1=2194, y1=3837, x2=2257, y2=3906\n",
      "  - Dimensions: 63x69 pixels\n",
      "  - Centre: (2226, 3871)\n",
      "\n",
      "Détection #2:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.932\n",
      "  - Bounding box: x1=1761, y1=1243, x2=1821, y2=1311\n",
      "  - Dimensions: 60x68 pixels\n",
      "  - Centre: (1791, 1277)\n",
      "\n",
      "Détection #3:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.925\n",
      "  - Bounding box: x1=2095, y1=870, x2=2162, y2=930\n",
      "  - Dimensions: 67x60 pixels\n",
      "  - Centre: (2128, 900)\n",
      "\n",
      "Détection #4:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.925\n",
      "  - Bounding box: x1=2816, y1=2330, x2=2877, y2=2397\n",
      "  - Dimensions: 61x67 pixels\n",
      "  - Centre: (2847, 2363)\n",
      "\n",
      "Détection #5:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.924\n",
      "  - Bounding box: x1=2766, y1=4686, x2=2832, y2=4747\n",
      "  - Dimensions: 66x60 pixels\n",
      "  - Centre: (2799, 4716)\n",
      "\n",
      "Détection #6:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.922\n",
      "  - Bounding box: x1=4479, y1=3067, x2=4543, y2=3129\n",
      "  - Dimensions: 64x62 pixels\n",
      "  - Centre: (4511, 3098)\n",
      "\n",
      "Détection #7:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.921\n",
      "  - Bounding box: x1=1773, y1=2939, x2=1830, y2=3002\n",
      "  - Dimensions: 58x63 pixels\n",
      "  - Centre: (1802, 2970)\n",
      "\n",
      "Détection #8:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.919\n",
      "  - Bounding box: x1=4247, y1=3177, x2=4310, y2=3245\n",
      "  - Dimensions: 63x68 pixels\n",
      "  - Centre: (4279, 3211)\n",
      "\n",
      "Détection #9:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.919\n",
      "  - Bounding box: x1=3752, y1=2561, x2=3811, y2=2619\n",
      "  - Dimensions: 59x58 pixels\n",
      "  - Centre: (3782, 2590)\n",
      "\n",
      "Détection #10:\n",
      "  - Classe: varroa\n",
      "  - Confiance: 0.917\n",
      "  - Bounding box: x1=2083, y1=4202, x2=2148, y2=4260\n",
      "  - Dimensions: 65x58 pixels\n",
      "  - Centre: (2116, 4231)\n"
     ]
    }
   ],
   "source": [
    "# Détection avec SAHI (Slicing Aided Hyper Inference)\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction\n",
    "from sahi.utils.cv import read_image\n",
    "import os\n",
    "\n",
    "# Charger le modèle via SAHI\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type=\"yolov8\",  # Compatible avec YOLO11\n",
    "    model_path=\"model_mdpi_3291496/weights/best.pt\",\n",
    "    confidence_threshold=0.1,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Image à analyser\n",
    "image_path = 'varroa-counter-v3-3/test/images/IMG_6098_jpg.rf.eb517a2863fbb704a84afce55b287455.jpg'\n",
    "\n",
    "# Effectuer la prédiction par découpage\n",
    "result = get_sliced_prediction(\n",
    "    image_path,\n",
    "    detection_model,\n",
    "    slice_height=4096,\n",
    "    slice_width=4096,\n",
    "    overlap_height_ratio=0.2,\n",
    "    overlap_width_ratio=0.2,\n",
    "    postprocess_type=\"NMS\",\n",
    "    postprocess_match_metric=\"IOU\",\n",
    "    postprocess_match_threshold=0.5\n",
    ")\n",
    "\n",
    "# Créer le dossier de résultats\n",
    "output_dir = \"results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Exporter l'image avec SEULEMENT les bounding boxes (sans texte de classe)\n",
    "result.export_visuals(\n",
    "    export_dir=output_dir,\n",
    "    file_name=\"detection_result\",\n",
    "    hide_labels=True,      # Masque le texte de classe\n",
    "    hide_conf=True,        # Masque le score de confiance\n",
    "    rect_th=1              # Épaisseur des rectangles\n",
    ")\n",
    "\n",
    "print(f\"Image exportée dans: {output_dir}/detection_result.png\")\n",
    "\n",
    "# ============================================================\n",
    "# INFORMATIONS DÉTAILLÉES SUR LES RÉSULTATS\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RÉSUMÉ DE LA DÉTECTION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "object_predictions = result.object_prediction_list\n",
    "print(f\"\\nNombre total de varroas détectés: {len(object_predictions)}\")\n",
    "\n",
    "# Statistiques sur les scores de confiance\n",
    "if object_predictions:\n",
    "    confidences = [pred.score.value for pred in object_predictions]\n",
    "    print(f\"\\nStatistiques de confiance:\")\n",
    "    print(f\"  - Confiance moyenne: {sum(confidences)/len(confidences):.3f}\")\n",
    "    print(f\"  - Confiance min: {min(confidences):.3f}\")\n",
    "    print(f\"  - Confiance max: {max(confidences):.3f}\")\n",
    "    \n",
    "    # Distribution par tranches de confiance\n",
    "    print(f\"\\nDistribution par niveau de confiance:\")\n",
    "    ranges = [(0.1, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
    "    for low, high in ranges:\n",
    "        count = sum(1 for c in confidences if low <= c < high)\n",
    "        print(f\"  - {low:.1f} - {high:.1f}: {count} détections\")\n",
    "\n",
    "    # Statistiques sur les tailles des bounding boxes\n",
    "    areas = []\n",
    "    for pred in object_predictions:\n",
    "        bbox = pred.bbox\n",
    "        width = bbox.maxx - bbox.minx\n",
    "        height = bbox.maxy - bbox.miny\n",
    "        areas.append(width * height)\n",
    "    \n",
    "    print(f\"\\nStatistiques des bounding boxes (en pixels):\")\n",
    "    print(f\"  - Aire moyenne: {sum(areas)/len(areas):.1f} px²\")\n",
    "    print(f\"  - Aire min: {min(areas):.1f} px²\")\n",
    "    print(f\"  - Aire max: {max(areas):.1f} px²\")\n",
    "\n",
    "# Afficher les détails des 10 premières détections\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DÉTAILS DES 10 PREMIÈRES DÉTECTIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for i, pred in enumerate(object_predictions[:10]):\n",
    "    bbox = pred.bbox\n",
    "    print(f\"\\nDétection #{i+1}:\")\n",
    "    print(f\"  - Classe: {pred.category.name}\")\n",
    "    print(f\"  - Confiance: {pred.score.value:.3f}\")\n",
    "    print(f\"  - Bounding box: x1={bbox.minx:.0f}, y1={bbox.miny:.0f}, x2={bbox.maxx:.0f}, y2={bbox.maxy:.0f}\")\n",
    "    print(f\"  - Dimensions: {bbox.maxx - bbox.minx:.0f}x{bbox.maxy - bbox.miny:.0f} pixels\")\n",
    "    print(f\"  - Centre: ({(bbox.minx + bbox.maxx)/2:.0f}, {(bbox.miny + bbox.maxy)/2:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les résultats détaillés en CSV\n",
    "import csv\n",
    "\n",
    "csv_path = os.path.join(output_dir, \"detections.csv\")\n",
    "\n",
    "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'classe', 'confiance', 'x1', 'y1', 'x2', 'y2', 'largeur', 'hauteur', 'aire', 'centre_x', 'centre_y'])\n",
    "    \n",
    "    for i, pred in enumerate(object_predictions):\n",
    "        bbox = pred.bbox\n",
    "        width = bbox.maxx - bbox.minx\n",
    "        height = bbox.maxy - bbox.miny\n",
    "        area = width * height\n",
    "        center_x = (bbox.minx + bbox.maxx) / 2\n",
    "        center_y = (bbox.miny + bbox.maxy) / 2\n",
    "        \n",
    "        writer.writerow([\n",
    "            i + 1,\n",
    "            pred.category.name,\n",
    "            f\"{pred.score.value:.4f}\",\n",
    "            f\"{bbox.minx:.1f}\",\n",
    "            f\"{bbox.miny:.1f}\",\n",
    "            f\"{bbox.maxx:.1f}\",\n",
    "            f\"{bbox.maxy:.1f}\",\n",
    "            f\"{width:.1f}\",\n",
    "            f\"{height:.1f}\",\n",
    "            f\"{area:.1f}\",\n",
    "            f\"{center_x:.1f}\",\n",
    "            f\"{center_y:.1f}\"\n",
    "        ])\n",
    "\n",
    "print(f\"Résultats exportés en CSV: {csv_path}\")\n",
    "print(f\"\\nLe fichier contient {len(object_predictions)} détections avec:\")\n",
    "print(\"  - ID unique\")\n",
    "print(\"  - Classe détectée\")\n",
    "print(\"  - Score de confiance\")\n",
    "print(\"  - Coordonnées du bounding box (x1, y1, x2, y2)\")\n",
    "print(\"  - Dimensions (largeur, hauteur)\")\n",
    "print(\"  - Aire en pixels²\")\n",
    "print(\"  - Coordonnées du centre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33mVarroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Charger l'image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m width, height = img.size\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTaille originale de l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mimage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'"
     ]
    }
   ],
   "source": [
    "# Test avec découpage d'image en 4 parties\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Charger le modèle\n",
    "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
    "\n",
    "# Chemin de l'image à tester\n",
    "image_path = 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'\n",
    "\n",
    "# Charger l'image\n",
    "img = Image.open(image_path)\n",
    "width, height = img.size\n",
    "print(f\"Taille originale de l'image: {width}x{height}\")\n",
    "\n",
    "# Créer un dossier pour les images découpées\n",
    "output_dir = 'temp_tiles'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Découper l'image en 4 parties (2x2)\n",
    "half_width = width // 2\n",
    "half_height = height // 2\n",
    "\n",
    "tiles = []\n",
    "positions = [\n",
    "    (0, 0, half_width, half_height, \"top_left\"),\n",
    "    (half_width, 0, width, half_height, \"top_right\"),\n",
    "    (0, half_height, half_width, height, \"bottom_left\"),\n",
    "    (half_width, half_height, width, height, \"bottom_right\")\n",
    "]\n",
    "\n",
    "# Découper et sauvegarder chaque partie\n",
    "for i, (x1, y1, x2, y2, name) in enumerate(positions):\n",
    "    tile = img.crop((x1, y1, x2, y2))\n",
    "    tile_path = os.path.join(output_dir, f'tile_{i+1}_{name}.jpg')\n",
    "    tile.save(tile_path)\n",
    "    tiles.append((tile_path, name, x1, y1))\n",
    "    print(f\"Partie {i+1} ({name}): {tile.size[0]}x{tile.size[1]} sauvegardée\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DÉTECTION SUR CHAQUE PARTIE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Tester le modèle sur chaque partie\n",
    "total_detections = 0\n",
    "all_results = []\n",
    "\n",
    "for tile_path, name, offset_x, offset_y in tiles:\n",
    "    print(f\"\\n--- Détection sur {name} ---\")\n",
    "    results = model.predict(\n",
    "        source=tile_path,\n",
    "   imgsz=(6016), max_det=2000, conf=0.1, iou = 0.5,\n",
    "    save=True, show_labels=False, line_width=2, save_txt=True, save_conf=True\n",
    "    )\n",
    "    \n",
    "    detections = len(results[0].boxes)\n",
    "    total_detections += detections\n",
    "    all_results.append((name, detections, results[0]))\n",
    "    \n",
    "    print(f\"Varroas détectés: {detections}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RÉSUMÉ DES DÉTECTIONS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nNombre total de varroas détectés: {total_detections}\")\n",
    "print(\"\\nDétail par partie:\")\n",
    "for name, count, _ in all_results:\n",
    "    print(f\"  - {name:15s}: {count:3d} varroas\")\n",
    "\n",
    "print(f\"\\nImages découpées sauvegardées dans: {output_dir}/\")\n",
    "print(f\"Résultats de détection sauvegardés dans: runs/detect/predict*/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def create_filtered_dataset(source_dir, dest_dir, prefix=\"IMG_6\"):\n",
    "    \"\"\"\n",
    "    Crée une copie d'un dataset YOLO en ne gardant que les images\n",
    "    dont le nom commence par le préfixe spécifié.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Chemin du dataset source (ex: \"varroa-counter-v3-3\")\n",
    "        dest_dir: Chemin du dataset destination\n",
    "        prefix: Préfixe des images à conserver (ex: \"IMG_6\")\n",
    "    \"\"\"\n",
    "    source_dir = Path(source_dir)\n",
    "    dest_dir = Path(dest_dir)\n",
    "\n",
    "    # Copier data.yaml et fichiers README\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for f in source_dir.glob(\"*.yaml\"):\n",
    "        shutil.copy2(f, dest_dir / f.name)\n",
    "    for f in source_dir.glob(\"*.txt\"):\n",
    "        shutil.copy2(f, dest_dir / f.name)\n",
    "\n",
    "    total_copied = 0\n",
    "\n",
    "    # Parcourir les splits (train, valid, test)\n",
    "    for split_dir in source_dir.iterdir():\n",
    "        if not split_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        images_dir = split_dir / \"images\"\n",
    "        labels_dir = split_dir / \"labels\"\n",
    "\n",
    "        if not images_dir.exists():\n",
    "            continue\n",
    "\n",
    "        # Créer les dossiers de destination\n",
    "        dest_images = dest_dir / split_dir.name / \"images\"\n",
    "        dest_labels = dest_dir / split_dir.name / \"labels\"\n",
    "        dest_images.mkdir(parents=True, exist_ok=True)\n",
    "        dest_labels.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Copier les images correspondant au préfixe et leurs labels\n",
    "        for img_file in images_dir.iterdir():\n",
    "            if img_file.name.startswith(prefix):\n",
    "                shutil.copy2(img_file, dest_images / img_file.name)\n",
    "\n",
    "                # Copier le label correspondant (.txt)\n",
    "                label_file = labels_dir / (img_file.stem + \".txt\")\n",
    "                if label_file.exists():\n",
    "                    shutil.copy2(label_file, dest_labels / label_file.name)\n",
    "\n",
    "                total_copied += 1\n",
    "\n",
    "    print(f\"Dataset filtré créé dans: {dest_dir}\")\n",
    "    print(f\"Images copiées: {total_copied}\")\n",
    "\n",
    "    # Lister le contenu\n",
    "    for split_dir in sorted(dest_dir.iterdir()):\n",
    "        if split_dir.is_dir():\n",
    "            imgs = list((split_dir / \"images\").glob(\"*\"))\n",
    "            print(f\"  {split_dir.name}: {len(imgs)} images\")\n",
    "\n",
    "\n",
    "# Créer le dataset filtré avec uniquement les images IMG_6xxx\n",
    "create_filtered_dataset(\n",
    "    source_dir=\"varroa-counter-v3-3\",\n",
    "    dest_dir=\"varroa-counter-v3-3-IMG6\",\n",
    "    prefix=\"IMG_6\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}