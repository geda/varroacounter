{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-2C4MRLFLuV"
      },
      "source": [
        "# Varroa Counter\n",
        "## 1.  DÃ©finition du problÃ¨me\n",
        "\n",
        "Les colonies d'abeilles du monde entier sont infestÃ©es par un parasite qui s'appelle le varroa destructor.\n",
        "Ce parasite au nom barbare se fixe sur le corps des abeilles adultes et se nourrit de l'hÃ©molymphe. Les femelles pÃ©nÃ¨trent aussi dans les cellules operculÃ©es pour se reproduire sur les larves, ce qui crÃ©e plusieurs gÃ©nÃ©rations au sein d'une mÃªme cellule.\n",
        "Le varroa transmet des virus aux abeilles et affaiblit leur systÃ¨me immunitaire.\n",
        "Si rien n'est entrepris pour stopper leur prolifÃ©ration, les colonies finissent par s'effondrer durant l'automne ou l'hiver.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/geda/varroacounter/blob/main/varroa_destructor.jpg?raw=1\" width=\"50%\">\n",
        "\n",
        "Une fois la rÃ©colte du miel effectuÃ©e, les apiculteurs effectuent diffÃ©rents traitements sur les colonies, notamment en utilisant de l'acide formique et de l'acide oxalique.\n",
        "Une fois le traitement effectuÃ©, l'apiculteur dÃ©pose une planchette sous la ruche afin d'Ã©valuer le degrÃ© d'infestation des colonies. Quelques jours aprÃ¨s le traitement, les varroas morts tombent sur le fond de la ruche.\n",
        "Les varroas ayant une taille de 1 Ã  2 mm, il devient trÃ¨s difficile de les compter lorsqu'ils sont nombreux. De plus, des rÃ©sidus de cires d'abeilles tombent Ã©galement des cadres, ce qui complique encore plus le comptage.\n",
        "\n",
        "<img src=\"https://github.com/geda/varroacounter/blob/main/fond_varroas.jpg?raw=1\" width=\"50%\">\n",
        "\n",
        "Les varroas sont les petites formes sombres allongÃ©es et arrondies.\n",
        "\n",
        "L'objectif de ce projet est d'estimer automatiquement le niveau d'infestation par les varroas Ã  partir d'une image. Il ne s'agit pas d'obtenir un dÃ©compte exact, notamment lorsque les varroas sont peu nombreux, mais plutÃ´t de fournir un ordre de grandeur fiable en cas d'infestation importante â€” par exemple, distinguer une image contenant 50 varroas d'une autre en contenant 200. Ce type d'Ã©valuation, difficile et fastidieux Ã  rÃ©aliser manuellement, est ainsi automatisÃ© pour faciliter le travail de l'apiculteur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0ce0fDyFLuZ"
      },
      "source": [
        "## 2. Collecte de donnÃ©es\n",
        "J'aimerais pouvoir simplement faire une photo de la planche complete et donnÃ© cette image assez large Ã  un modÃ¨le pour l'infÃ©rence.\n",
        "J'ai donc recherchÃ© des sets de donnÃ©es sur les varroas et j'en ai trouvÃ© plusieurs disponibles sur https://universe.roboflow.com/. Malheureusement aucun dataset ne correspondait parfaitement Ã  mes besoins:\n",
        "* Images de varroas sur les abeilles et non sur la planche\n",
        "* Images d'entraÃ®nement trop petites\n",
        "* Images avec des varroas labellisÃ©s mais qui ne ressemblent pas vraiment Ã  des varroas\n",
        "\n",
        "J'ai donc crÃ©Ã© un dataset avec mes propres images et j'ai uploadÃ© le dataset sur roboflow sous le projet suivant: https://app.roboflow.com/varroa-counter/varroa-counter-large/2\n",
        "\n",
        "### Inspection des donnÃ©es\n",
        "Le dataset de donnÃ©es comprend des\n",
        "* 32 images d'entraÃ®nement (71%)\n",
        "* 13 images de validation (19%)\n",
        "* 7 images de test (12%)\n",
        "\n",
        "## 3. PrÃ©paration des DonnÃ©es\n",
        "J'ai labellisÃ© les 32 images en identifiant plus 1000 varroas. J'ai effectuÃ© ce travail trÃ¨s chronophage directement sur roboflow.\n",
        "\n",
        "\n",
        "Chaque image de ce set a donc maintenant un label associÃ©. Un seul nom de classe est utilisÃ© pour ce dataset: **varroa**\n",
        "Les labels associÃ©s Ã  une image sont sauvegardÃ©s au format YOLO et comprennent simplement une suite de nombres comme ceci:\n",
        "* 0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
        "* 0 0.021484375 0.27880859375 0.009765625 0.0166015625\n",
        "* 0 0.0751953125 0.3349609375 0.009765625 0.015625\n",
        "* ...\n",
        "\n",
        "#### Explication du format YOLO\n",
        "\n",
        "```\n",
        "0 0.02587890625 0.25439453125 0.0107421875 0.0166015625\n",
        "â”‚      â”‚              â”‚            â”‚            â”‚\n",
        "â”‚      â”‚              â”‚            â”‚            â””â”€â”€ height (hauteur normalisÃ©e de la bounding box)\n",
        "â”‚      â”‚              â”‚            â””â”€â”€ width (largeur normalisÃ©e de la bounding box)\n",
        "â”‚      â”‚              â””â”€â”€ y_center (position Y du centre, normalisÃ©e)\n",
        "â”‚      â””â”€â”€ x_center (position X du centre, normalisÃ©e)\n",
        "â””â”€â”€ class_id (identifiant de la classe = 0 = varroa)\n",
        "```\n",
        "\n",
        "| Valeur | Signification | Exemple |\n",
        "|--------|---------------|---------|\n",
        "| `0` | ID de la classe | varroa (seule classe du dataset) |\n",
        "| `0.0259` | x_center | Le centre est Ã  2.6% de la largeur de l'image (trÃ¨s Ã  gauche) |\n",
        "| `0.2544` | y_center | Le centre est Ã  25.4% de la hauteur de l'image |\n",
        "| `0.0107` | width | La box fait 1.07% de la largeur de l'image |\n",
        "| `0.0166` | height | La box fait 1.66% de la hauteur de l'image |\n",
        "\n",
        "## 4. Analyse Exploratoire des DonnÃ©es (AED)\n",
        "Les donnÃ©es contiennent des images avec des fonds de diffÃ©rentes couleurs et matiÃ¨res.\n",
        "\n",
        "## 5. Feature Engineering\n",
        "Le modÃ¨le devra faire de la dÃ©tection d'objets. Un des challenges sera de dÃ©tecter des objets trÃ¨s petits dans les images.\n",
        "Les 2 features importantes de la dÃ©tection d'objets seront:\n",
        "* Classification d'image: dÃ©terminer si des varroas sont prÃ©sents dans l'image\n",
        "* Localisation d'objet: trouver la position des varroas Ã  l'aide de _bounding boxes_\n",
        "\n",
        "## 6. ModÃ©lisation\n",
        "Je choisi la classification comme modÃ¨le d'apprentissage.\n",
        "\n",
        "Je divise mon ensemble de donnÃ©e comme ceci:\n",
        "* 26 images d'entraÃ®nement (80%)\n",
        "* 4 images de validation (12%)\n",
        "* 2 images de test (8%)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/geda/varroacounter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PljkTsfFGdlu",
        "outputId": "c2fa157f-5c13-4977-c5b3-54d0590e8007"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'varroacounter'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 40 (delta 18), reused 29 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 11.91 MiB | 19.26 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yosvmv9_FLua",
        "outputId": "6e5201b7-aad1-452c-ec1f-ad701c03a591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.13-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from roboflow) (2026.1.4)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.12/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.32.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.12/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from roboflow) (4.67.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from roboflow) (6.0.3)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.12/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (4.61.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->roboflow) (3.3.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->roboflow) (3.4.4)\n",
            "Downloading roboflow-1.2.13-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.5-cp312-cp312-manylinux_2_28_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, pi-heif, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.13.0.90\n",
            "    Uninstalling opencv-python-headless-4.13.0.90:\n",
            "      Successfully uninstalled opencv-python-headless-4.13.0.90\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.11\n",
            "    Uninstalling idna-3.11:\n",
            "      Successfully uninstalled idna-3.11\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.2.0 pillow-avif-plugin-1.5.5 roboflow-1.2.13\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in varroa-counter-large-3 to yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42361/42361 [00:03<00:00, 13385.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to varroa-counter-large-3 in yolov11:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62/62 [00:00<00:00, 921.71it/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "import os\n",
        "from roboflow import Roboflow\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('ROBOFLOW_API_KEY')\n",
        "except ImportError:\n",
        "    api_key = os.getenv(\"ROBOFLOW_API_KEY\")\n",
        "\n",
        "rf = Roboflow(api_key=api_key)\n",
        "\n",
        "\n",
        "project = rf.workspace(\"varroa-counter\").project(\"varroa-counter-large\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1bO6gCXFLuc"
      },
      "source": [
        "## 7. EntraÃ®nement du modÃ¨le\n",
        "### ğŸ“Œ Version de Python recommandÃ©e\n",
        "\n",
        "Pour cet exercice de rÃ©seaux de neurones convolutifs (CNN) avec YOLO11n, je vais utiliser **Python 3.12.7**, car câ€™est lâ€™une des versions les mieux supportÃ©es par YOLO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpJau5CIFLuc",
        "outputId": "13c50efc-abbc-47b9-d85d-d81432d1ec9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "/usr/bin/python3\n"
          ]
        }
      ],
      "source": [
        "# VÃ©rifions la version de Python et du chemin de l'exÃ©cutable\n",
        "import sys\n",
        "print(sys.version)\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j03qiL5FFLuc",
        "outputId": "8a47d6c4-3230-4d58-c7b3-3599f97ffc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "invalid escape sequence '\\d'\n",
            "invalid escape sequence '\\d'\n",
            "invalid escape sequence '\\d'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.12-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.90)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.12-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.12 ultralytics-thop-2.0.18\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo11n.pt to 'yolo11n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.4MB 69.6MB/s 0.1s\n",
            "Ultralytics 8.4.12 ğŸš€ Python-3.12.12 torch-2.9.0+cpu CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=0.1, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=varroa-counter-large-3\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=6016, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=2000, mixup=0.0, mode=train, model=yolo11n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Dataset 'varroa-counter-large-3\\data.yaml' error âŒ 'varroa-counter-large-3\\data.yaml' does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    659\u001b[0m             }:\n\u001b[0;32m--> 660\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_det_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"yaml_file\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/data/utils.py\u001b[0m in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \"\"\"\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/utils/checks.py\u001b[0m in \u001b[0;36mcheck_file\u001b[0;34m(file, suffix, download, download_dir, hard)\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{file}' does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: 'varroa-counter-large-3\\data.yaml' does not exist",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3044360343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# lancement de l'entraÃ®nement avec seulement 1 seul passage et en abaissant le nombre de batch (nombre d'images traitÃ©es simultanÃ©ment).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# le but est de tester si les capacitÃ©es de ma machine sont suffisantes pour entraÃ®ner mon modÃ¨le\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m results = model.train(data=\"varroa-counter-large-3\\data.yaml\", epochs=1, imgsz=6016, \n\u001b[0m\u001b[1;32m     13\u001b[0m                       max_det=2000, conf=0.1, iou = 0.5)\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resume\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# manually set model only if not resuming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/models/yolo/detect/train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0m_callbacks\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mexecuted\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \"\"\"\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_model_file_from_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add suffix, i.e. yolo26n -> yolo26n.pt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch_distributed_zero_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_RANK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# avoid auto-downloading dataset multiple times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    662\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"yaml_file\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# for validating 'yolo train data=url.zip' usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memojis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{clean_url(self.args.data)}' error âŒ {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overriding class names with single class.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset 'varroa-counter-large-3\\data.yaml' error âŒ 'varroa-counter-large-3\\data.yaml' does not exist"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "# import the needed librairies\n",
        "import ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "# entrainement du modÃ¨le\n",
        "# Je crÃ©er un nouveau modÃ¨le depuis zÃ©ro\n",
        "model = YOLO('yolo11n.pt')\n",
        "\n",
        "# lancement de l'entraÃ®nement avec seulement 1 seul passage et en abaissant le nombre de batch (nombre d'images traitÃ©es simultanÃ©ment).\n",
        "# le but est de tester si les capacitÃ©es de ma machine sont suffisantes pour entraÃ®ner mon modÃ¨le\n",
        "results = model.train(data=\"varroa-counter-large-3\\data.yaml\", epochs=1, imgsz=6016,\n",
        "                      max_det=2000, conf=0.1, iou = 0.5)\n",
        "print (results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph6EqtZ0FLud"
      },
      "source": [
        "Impossible d'entraÃ®ner mon modÃ¨le sur ma machine, car elle n'a pas de GPU et le kernel crash.\n",
        "J'ai fait un essai en utilisant google colab, qui permet gratuitement d'entraÃ®ner des modÃ¨les avec un peu de GPU. Malheureusement j'ai le mÃªme problÃ¨me sur google colab qui m'indique que ma session a plantÃ© aprÃ¨s avoir utilisÃ© toute la RAM disponible.\n",
        "\n",
        "Le problÃ¨me est l'entraÃ®nement de modÃ¨le avec des images de grandes tailles car cela nÃ©cessite beaucoup de mÃ©moire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ia2HfVFLud"
      },
      "source": [
        "### Changement de stratÃ©gie: recherche d'un modÃ¨le existant\n",
        "Je dÃ©cide donc de rechercher un modÃ¨le existant pouvant couvrir mes besoins et j'ai trouvÃ© le travail de recherche suivant: https://www.mdpi.com/2077-0472/15/9/969\n",
        "\n",
        "**RÃ©fÃ©rence:**\n",
        "> YÃ¡niz, J.; Casalongue, M.; Martinez-de-Pison, F.J.; Silvestre, M.A.; Consortium, B.; Santolaria, P.; DivasÃ³n, J. *An AI-Based Open-Source Software for Varroa Mite Fall Analysis in Honeybee Colonies*. Agriculture 2025, 15, 969. https://doi.org/10.3390/agriculture15090969\n",
        "\n",
        "Leurs modÃ¨le a Ã©tÃ© entraÃ®nÃ© avec un dataset de 357 images sur plus de 500 epochs. Ils ont Ã©galement livrÃ© un programme Ã©crit en python permettant d'upload ses images et d'effectuer la dÃ©tection des varroas avec leurs modÃ¨le.\n",
        "Le code source est disponible sous github ainsi que leurs modÃ¨le entraÃ®nÃ©: https://github.com/jodivaso/varrodetector/blob/main/model/weights/best.pt\n",
        "\n",
        "En analysant le code j'ai trouvÃ© particuliÃ¨rement intÃ©ressant qu'ils utilisent une taille d'image assez grande de 6016 pixels (https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507C42-L2507C46).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8evVCdrFLud"
      },
      "source": [
        "## 8. Evaluation du modÃ¨le\n",
        "Je dÃ©cide de rebalancer toutes les images de mon dataset en set de test et de n'appliquer aucun redimensionnement d'image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Olv1Q-cFLue",
        "outputId": "2307919b-74f4-46b0-826e-b241d74c40d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"tEQkmVJiCxGOZMuDLR6d\")\n",
        "project = rf.workspace(\"varroa-counter\").project(\"varroa-counter-v3\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F--qizN-FLue"
      },
      "source": [
        "Et de lancer le test du modÃ¨le avec mon dataset de test comprenant 32 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri0-ybbhFLue",
        "outputId": "9ef5b989-40a7-4095-ddc2-77c1897f62dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 632.795.6 MB/s, size: 1693.7 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-large-3\\train\\labels.cache... 19 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 19/19  0.0s\n",
            "WARNING Box and segment counts should be equal, but got len(segments) = 418, len(boxes) = 1595. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 29.9s/it 2:2940.4s2\n",
            "                   all         19       1595      0.414      0.401      0.352     0.0985\n",
            "Speed: 124.8ms preprocess, 6928.0ms inference, 0.0ms loss, 14.1ms postprocess per image\n",
            "Saving D:\\dev\\runs\\detect\\val70\\predictions.json...\n",
            "Results saved to \u001b[1mD:\\dev\\runs\\detect\\val70\u001b[0m\n",
            "Precision: [    0.41353]\n",
            "Recall: [    0.40125]\n",
            "mAP50: 0.3521537608756259\n",
            "mAP50-95: 0.09846878398398631\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "# import the needed librairies\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Charger le modÃ¨le entraÃ®nÃ©\n",
        "model = YOLO('varroacounter/model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# Effectuer la dÃ©tection sur l'image\n",
        "results = model.val(\n",
        "    data=\"varroa-counter-large-3/data.yaml\",\n",
        "    split='train',  # or 'val' for validation set\n",
        "    imgsz=6016, # mÃªme valeur celle utilisÃ©e dans le code https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507\n",
        "    batch=4,\n",
        "    conf=0.1,  # Seuil de confiance\n",
        "    iou=0.5,\n",
        "    max_det=2000,\n",
        "    save_json=True,\n",
        "    save=True,\n",
        "    show_labels=False,\n",
        "    show_conf=False,\n",
        "    line_width=2\n",
        ")\n",
        "\n",
        "# Afficher les rÃ©sultats\n",
        "# Print results\n",
        "print(f\"Precision: {results.box.p}\")\n",
        "print(f\"Recall: {results.box.r}\")\n",
        "print(f\"mAP50: {results.box.map50}\")\n",
        "print(f\"mAP50-95: {results.box.map}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocUToqm1FLue"
      },
      "source": [
        "*En rÃ©sumÃ© la qualitÃ© de ce modÃ¨le par rapport Ã  mon problÃ¨me est faible*\n",
        "\n",
        "**Precision : 0.4135 (41%)**\n",
        "Sur toutes les dÃ©tections faites par le modÃ¨le, seulement 41% sont correctes. Cela signifie que ~59% des dÃ©tections sont des faux positifs (le modÃ¨le dÃ©tecte un varroa lÃ  oÃ¹ il n'y en a pas).\n",
        "\n",
        "**Recall : 0.4013 (40%)**\n",
        "Sur tous les varroas rÃ©ellement prÃ©sents dans les images, le modÃ¨le n'en dÃ©tecte que 40%. Il rate donc ~60% des varroas rÃ©els.\n",
        "\n",
        "**mAP50 : 0.352**\n",
        "La prÃ©cision moyenne (mean Average Precision) avec un seuil IoU de 50%. C'est la mÃ©trique standard de performance en dÃ©tection d'objets. Un score de 0.35 est faible.\n",
        "\n",
        "**mAP50-95 : 0.098**\n",
        "La mÃªme mÃ©trique mais moyennÃ©e sur des seuils IoU de 50% Ã  95% (plus strict sur la prÃ©cision de localisation). Un score de ~0.10 est trÃ¨s faible, ce qui indique que mÃªme quand le modÃ¨le trouve un varroa, la boÃ®te englobante est souvent mal positionnÃ©e.\n",
        "\n",
        "Je dÃ©cide donc de faire un test avec 1 seule image afin de dÃ©terminer plus prÃ©cisÃ©ment oÃ¹ le problÃ¨me se trouve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZKEHE9QFLuf",
        "outputId": "fdc2c5c9-427e-495e-fe79-5eb40ca096d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 d:\\dev\\ia\\cours\\varroacounter\\varroa-counter-large-3\\train\\images\\IMG_0223_cropped_jpg.rf.0e4c7399e5f9e2e6287c52cd865bae46.jpg: 6016x4544 250 varroas, 4753.5ms\n",
            "Speed: 343.0ms preprocess, 4753.5ms inference, 27.0ms postprocess per image at shape (1, 3, 6016, 4544)\n",
            "Results saved to \u001b[1mD:\\dev\\runs\\detect\\predict16\u001b[0m\n",
            "Varroas dÃ©tectÃ©s: 250\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Charger le modÃ¨le entraÃ®nÃ©\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# Effectuer la dÃ©tection sur l'image\n",
        "results = model.predict(\n",
        "    source=\"varroa-counter-large-3/train/images/IMG_0223_cropped_jpg.rf.0e4c7399e5f9e2e6287c52cd865bae46.jpg\",\n",
        "    imgsz=6016,\n",
        "    max_det=2000,\n",
        "    conf=0.1,\n",
        "    iou=0.5,\n",
        "    save=True,\n",
        "    show_labels=False,\n",
        "    show_conf=False,\n",
        "    line_width=2\n",
        ")\n",
        "\n",
        "print(f\"Varroas dÃ©tectÃ©s: {len(results[0].boxes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDoqIhTtFLuf"
      },
      "source": [
        "Voici un zoom sur l'image gÃ©nÃ©rÃ©e par la dÃ©tection\n",
        "![image annotÃ© aprÃ¨s prÃ©diction](https://github.com/geda/varroacounter/blob/main/IMG_0223_cropped_predict_16_zoom.jpg?raw=1)\n",
        "\n",
        "On constate les problÃ¨mes suivants:\n",
        "- le modÃ¨le confond les gouttes d'eau et les varroas\n",
        "- le modÃ¨le oublie des varroas pourtant bien visibles\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASKcZZHcFLuf",
        "outputId": "60d4a287-c695-4147-e981-5e0b85fd0ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.12-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.90)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (26.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.12-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.12 ultralytics-thop-2.0.18\n",
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics 8.4.12 ğŸš€ Python-3.12.12 torch-2.9.0+cpu CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=0.1, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=varroa-counter-large-3/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=6016, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=2000, mixup=0.0, mode=train, model=varroacounter/model_mdpi_3291496/weights/best.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 4.6MB/s 0.2s\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "Freezing layer 'model.0.conv.weight'\n",
            "Freezing layer 'model.0.bn.weight'\n",
            "Freezing layer 'model.0.bn.bias'\n",
            "Freezing layer 'model.1.conv.weight'\n",
            "Freezing layer 'model.1.bn.weight'\n",
            "Freezing layer 'model.1.bn.bias'\n",
            "Freezing layer 'model.2.cv1.conv.weight'\n",
            "Freezing layer 'model.2.cv1.bn.weight'\n",
            "Freezing layer 'model.2.cv1.bn.bias'\n",
            "Freezing layer 'model.2.cv2.conv.weight'\n",
            "Freezing layer 'model.2.cv2.bn.weight'\n",
            "Freezing layer 'model.2.cv2.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.3.conv.weight'\n",
            "Freezing layer 'model.3.bn.weight'\n",
            "Freezing layer 'model.3.bn.bias'\n",
            "Freezing layer 'model.4.cv1.conv.weight'\n",
            "Freezing layer 'model.4.cv1.bn.weight'\n",
            "Freezing layer 'model.4.cv1.bn.bias'\n",
            "Freezing layer 'model.4.cv2.conv.weight'\n",
            "Freezing layer 'model.4.cv2.bn.weight'\n",
            "Freezing layer 'model.4.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.5.conv.weight'\n",
            "Freezing layer 'model.5.bn.weight'\n",
            "Freezing layer 'model.5.bn.bias'\n",
            "Freezing layer 'model.6.cv1.conv.weight'\n",
            "Freezing layer 'model.6.cv1.bn.weight'\n",
            "Freezing layer 'model.6.cv1.bn.bias'\n",
            "Freezing layer 'model.6.cv2.conv.weight'\n",
            "Freezing layer 'model.6.cv2.bn.weight'\n",
            "Freezing layer 'model.6.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
            "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
            "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
            "Freezing layer 'model.7.conv.weight'\n",
            "Freezing layer 'model.7.bn.weight'\n",
            "Freezing layer 'model.7.bn.bias'\n",
            "Freezing layer 'model.8.cv1.conv.weight'\n",
            "Freezing layer 'model.8.cv1.bn.weight'\n",
            "Freezing layer 'model.8.cv1.bn.bias'\n",
            "Freezing layer 'model.8.cv2.conv.weight'\n",
            "Freezing layer 'model.8.cv2.bn.weight'\n",
            "Freezing layer 'model.8.cv2.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
            "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
            "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
            "Freezing layer 'model.9.cv1.conv.weight'\n",
            "Freezing layer 'model.9.cv1.bn.weight'\n",
            "Freezing layer 'model.9.cv1.bn.bias'\n",
            "Freezing layer 'model.9.cv2.conv.weight'\n",
            "Freezing layer 'model.9.cv2.bn.weight'\n",
            "Freezing layer 'model.9.cv2.bn.bias'\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 6182.2Â±1492.9 MB/s, size: 1586.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/varroa-counter-large-3/train/labels... 19 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 19/19 257.0it/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/varroa-counter-large-3/train/labels.cache\n",
            "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 418, len(boxes) = 1595. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5434.0Â±723.1 MB/s, size: 1907.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/varroa-counter-large-3/valid/labels... 4 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 4/4 946.7it/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/varroa-counter-large-3/valid/labels.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 6016 train, 6016 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "# import the needed librairies\n",
        "import ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "# entrainement du modÃ¨le\n",
        "# Je crÃ©er un nouveau modÃ¨le depuis zÃ©ro\n",
        "model = YOLO('varroacounter/model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# lancement de l'entraÃ®nement avec seulement 1 seul passage et en abaissant le nombre de batch (nombre d'images traitÃ©es simultanÃ©ment).\n",
        "# le but est de tester si les capacitÃ©es de ma machine sont suffisantes pour entraÃ®ner mon modÃ¨le\n",
        "results = model.train(data=\"varroa-counter-large-3/data.yaml\",\n",
        "                      epochs=1,\n",
        "                      imgsz=6016,\n",
        "                      batch=16,\n",
        "                      freeze=10,\n",
        "                      max_det=2000,\n",
        "                      conf=0.1,\n",
        "                      iou = 0.5)\n",
        "print (results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdi1FtEeFLuf"
      },
      "source": [
        "Je dÃ©cide de faire une prÃ©diction valider le modÃ¨le avec uniquement les images provenant du site espagnol. Je ne sais pas si elles ont Ã©tÃ© utilisÃ©es lors de l'entraÃ®nement du modÃ¨le, mais dÃ©cide de faire une comparaison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGkzye2gFLuf",
        "outputId": "889848a1-5509-435b-a0b0-a072d54e11b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset filtrÃ© crÃ©Ã© dans: varroa-counter-v3-3-IMG6\n",
            "Images copiÃ©es: 5\n",
            "  test: 5 images\n",
            "  valid: 0 images\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def create_filtered_dataset(source_dir, dest_dir, prefix=\"IMG_6\"):\n",
        "    \"\"\"\n",
        "    CrÃ©e une copie d'un dataset YOLO en ne gardant que les images\n",
        "    dont le nom commence par le prÃ©fixe spÃ©cifiÃ©.\n",
        "\n",
        "    Args:\n",
        "        source_dir: Chemin du dataset source (ex: \"varroa-counter-v3-3\")\n",
        "        dest_dir: Chemin du dataset destination\n",
        "        prefix: PrÃ©fixe des images Ã  conserver (ex: \"IMG_6\")\n",
        "    \"\"\"\n",
        "    source_dir = Path(source_dir)\n",
        "    dest_dir = Path(dest_dir)\n",
        "\n",
        "    # Copier data.yaml et fichiers README\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for f in source_dir.glob(\"*.yaml\"):\n",
        "        shutil.copy2(f, dest_dir / f.name)\n",
        "    for f in source_dir.glob(\"*.txt\"):\n",
        "        shutil.copy2(f, dest_dir / f.name)\n",
        "\n",
        "    total_copied = 0\n",
        "\n",
        "    # Parcourir les splits (train, valid, test)\n",
        "    for split_dir in source_dir.iterdir():\n",
        "        if not split_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        images_dir = split_dir / \"images\"\n",
        "        labels_dir = split_dir / \"labels\"\n",
        "\n",
        "        if not images_dir.exists():\n",
        "            continue\n",
        "\n",
        "        # CrÃ©er les dossiers de destination\n",
        "        dest_images = dest_dir / split_dir.name / \"images\"\n",
        "        dest_labels = dest_dir / split_dir.name / \"labels\"\n",
        "        dest_images.mkdir(parents=True, exist_ok=True)\n",
        "        dest_labels.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Copier les images correspondant au prÃ©fixe et leurs labels\n",
        "        for img_file in images_dir.iterdir():\n",
        "            if img_file.name.startswith(prefix):\n",
        "                shutil.copy2(img_file, dest_images / img_file.name)\n",
        "\n",
        "                # Copier le label correspondant (.txt)\n",
        "                label_file = labels_dir / (img_file.stem + \".txt\")\n",
        "                if label_file.exists():\n",
        "                    shutil.copy2(label_file, dest_labels / label_file.name)\n",
        "\n",
        "                total_copied += 1\n",
        "\n",
        "    print(f\"Dataset filtrÃ© crÃ©Ã© dans: {dest_dir}\")\n",
        "    print(f\"Images copiÃ©es: {total_copied}\")\n",
        "\n",
        "    # Lister le contenu\n",
        "    for split_dir in sorted(dest_dir.iterdir()):\n",
        "        if split_dir.is_dir():\n",
        "            imgs = list((split_dir / \"images\").glob(\"*\"))\n",
        "            print(f\"  {split_dir.name}: {len(imgs)} images\")\n",
        "\n",
        "\n",
        "# CrÃ©er le dataset filtrÃ© avec uniquement les images IMG_6xxx\n",
        "create_filtered_dataset(\n",
        "    source_dir=\"varroa-counter-v3-3\",\n",
        "    dest_dir=\"varroa-counter-v3-3-IMG6\",\n",
        "    prefix=\"IMG_6\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuiVhXvRFLuf",
        "outputId": "0f41ea20-6f89-4cef-a002-dc209c2fceb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 85.96.4 MB/s, size: 1745.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3-IMG6\\test\\labels... 5 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 118.5it/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3-IMG6\\test\\labels.cache\n",
            "WARNING Box and segment counts should be equal, but got len(segments) = 209, len(boxes) = 745. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 123.3s/it 2:03\n",
            "                   all          5        745      0.609      0.483      0.483      0.148\n",
            "Speed: 890.8ms preprocess, 22679.2ms inference, 0.3ms loss, 49.0ms postprocess per image\n",
            "Saving D:\\dev\\runs\\detect\\val65\\predictions.json...\n",
            "Results saved to \u001b[1mD:\\dev\\runs\\detect\\val65\u001b[0m\n",
            "Precision: [    0.60893]\n",
            "Recall: [     0.4828]\n",
            "mAP50: 0.48298499787085747\n",
            "mAP50-95: 0.1482758188990101\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Charger le modÃ¨le entraÃ®nÃ©\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# Effectuer la dÃ©tection sur l'image\n",
        "results = model.val(\n",
        "    data=\"varroa-counter-v3-3-IMG6/data.yaml\",\n",
        "    split='test',  # or 'val' for validation set\n",
        "    imgsz=6016, # mÃªme valeur celle utilisÃ©e dans le code https://github.com/jodivaso/varrodetector/blob/main/varroa_mite_gui.py#L2507\n",
        "    batch=16,\n",
        "    conf=0.1,  # Seuil de confiance\n",
        "    iou=0.5,\n",
        "    max_det=2000,\n",
        "    save_json=True,\n",
        "    save=True,\n",
        "    show_labels=False,\n",
        "    show_conf=False,\n",
        "    line_width=2\n",
        ")\n",
        "\n",
        "# Afficher les rÃ©sultats\n",
        "# Print results\n",
        "print(f\"Precision: {results.box.p}\")\n",
        "print(f\"Recall: {results.box.r}\")\n",
        "print(f\"mAP50: {results.box.map50}\")\n",
        "print(f\"mAP50-95: {results.box.map}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-45ph0iWFLug"
      },
      "source": [
        "Ces rÃ©sultats montrent une performance amÃ©liorÃ©e par rapport Ã  la validation du set complet, mais quand-mÃªme une performance relativement faible.\n",
        "\n",
        "Precision (0.61) â€” Sur toutes les dÃ©tections faites, 61% sont correctes. Environ 4 dÃ©tections sur 10 sont des faux positifs.\n",
        "\n",
        "Recall (0.48) â€” Le modÃ¨le ne dÃ©tecte que 48% des varroas rÃ©ellement prÃ©sents. Il en rate plus de la moitiÃ©.\n",
        "\n",
        "mAP50 (0.48) â€” Performance globale Ã  IoU 50% insuffisante. Un modÃ¨le correct vise >0.5, un bon modÃ¨le >0.7.\n",
        "\n",
        "mAP50-95 (0.15) â€” La localisation prÃ©cise est trÃ¨s faible. MÃªme quand le modÃ¨le trouve un varroa, la bounding box est souvent mal positionnÃ©e."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pothDG9jFLug",
        "outputId": "7743d5e0-4201-4dc5-99d7-dc35f0f5b282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/1 d:\\dev\\ia\\cours\\varroacounter\\varroas\\Sample images\\IMG_6187.jpg: 6016x4512 85 varroas, 7622.3ms\n",
            "Speed: 525.4ms preprocess, 7622.3ms inference, 28.9ms postprocess per image at shape (1, 3, 6016, 4512)\n",
            "Results saved to \u001b[1mD:\\dev\\runs\\detect\\predict15\u001b[0m\n",
            "Varroas dÃ©tectÃ©s: 85\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Charger le modÃ¨le entraÃ®nÃ©\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# Effectuer la dÃ©tection sur l'image\n",
        "results = model.predict(\n",
        "    source=\"varroas/Sample images/IMG_6187.jpg\",\n",
        "    imgsz=6016,\n",
        "    max_det=2000,\n",
        "    conf=0.1,\n",
        "    iou=0.5,\n",
        "    save=True,\n",
        "    show_labels=False,\n",
        "    show_conf=False,\n",
        "    line_width=2\n",
        ")\n",
        "\n",
        "print(f\"Varroas dÃ©tectÃ©s: {len(results[0].boxes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYXORk3FLug"
      },
      "source": [
        "On constate sur l'image rÃ©sultant de la prÃ©diction les problÃ¨mes suivants:\n",
        "- lorsque le varroa est posÃ© sur de la cire, il est mal dÃ©tectÃ©\n",
        "- lorsque 2 varroas sont cÃ´te Ã  cÃ´te, parfois un seul varroa est dÃ©tectÃ©\n",
        "\n",
        "![image de l'Ã©tude annotÃ©](https://github.com/geda/varroacounter/blob/main/IMG_6187_predict_15_zoom.jpg?raw=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_0NIrW1FLug",
        "outputId": "b0cc2847-f8e6-4e54-81a3-e9b475a1bfe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sahi in d:\\dev\\python\\python312\\lib\\site-packages (0.11.36)\n",
            "Requirement already satisfied: click in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (8.3.1)\n",
            "Requirement already satisfied: fire in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (0.7.1)\n",
            "Requirement already satisfied: opencv-python<=4.11.0.86 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=8.2.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (12.0.0)\n",
            "Requirement already satisfied: pybboxes==0.1.6 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (0.1.6)\n",
            "Requirement already satisfied: pyyaml in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (6.0.3)\n",
            "Requirement already satisfied: requests in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.32.5)\n",
            "Requirement already satisfied: shapely>=2.0.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.1.2)\n",
            "Requirement already satisfied: terminaltables in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (3.1.10)\n",
            "Requirement already satisfied: torch>=2.4.1 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (2.9.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in d:\\dev\\python\\python312\\lib\\site-packages (from sahi) (4.67.1)\n",
            "Requirement already satisfied: numpy in d:\\dev\\python\\python312\\lib\\site-packages (from pybboxes==0.1.6->sahi) (2.2.6)\n",
            "Requirement already satisfied: filelock in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (2025.12.0)\n",
            "Requirement already satisfied: setuptools in d:\\dev\\python\\python312\\lib\\site-packages (from torch>=2.4.1->sahi) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\dev\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.4.1->sahi) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\david\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.48.2->sahi) (0.4.6)\n",
            "Requirement already satisfied: termcolor in d:\\dev\\python\\python312\\lib\\site-packages (from fire->sahi) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\dev\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.4.1->sahi) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\dev\\python\\python312\\lib\\site-packages (from requests->sahi) (2025.11.12)\n",
            "Performing prediction on 35 slices.\n"
          ]
        }
      ],
      "source": [
        "!pip install sahi\n",
        "\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "\n",
        "model = AutoDetectionModel.from_pretrained(\n",
        "    model_type=\"yolov8\",  # Compatible YOLO11\n",
        "    model_path=\"model_mdpi_3291496/weights/best.pt\",\n",
        "    confidence_threshold=0.1\n",
        ")\n",
        "\n",
        "result = get_sliced_prediction(\n",
        "    image=\"varroa-counter-v3-3/test/images/IMG_0223_jpg.rf.a0db71f36b6b62fd38a1deca87b0a1f0.jpg\",\n",
        "    detection_model=model,\n",
        "    slice_height=1024,\n",
        "    slice_width=1024,\n",
        "    overlap_height_ratio=0.2,\n",
        "    overlap_width_ratio=0.2\n",
        ")\n",
        "\n",
        "result.export_visuals(export_dir=\"results/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElsygeOCFLug"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "results = model.train(\n",
        "    data=\"varroa-counter-v3-2/data.yaml\",\n",
        "    epochs=150,\n",
        "    imgsz=2048,\n",
        "    batch=4,\n",
        "    freeze=10  # GÃ¨le les premiÃ¨res couches, ajuste seulement les derniÃ¨res\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYwouzqBFLug",
        "outputId": "3f4dee3d-e191-4a53-ca0f-174fdd4cd08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 138.130.7 MB/s, size: 1597.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3\\test\\labels... 32 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 32/32 228.3it/s 0.1s.2s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\dev\\ia\\cours\\varroacounter\\varroa-counter-v3-3\\test\\labels.cache\n",
            "WARNING Box and segment counts should be equal, but got len(segments) = 209, len(boxes) = 1057. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 14.3s/it 28.6s49.4s\n",
            "                   all         32       1057      0.566      0.459       0.51      0.268\n",
            "Speed: 18.7ms preprocess, 592.8ms inference, 0.0ms loss, 2.2ms postprocess per image\n",
            "Results saved to \u001b[1mD:\\dev\\runs\\detect\\val34\u001b[0m\n",
            "Precision: [    0.56554]\n",
            "Recall: [    0.45885]\n",
            "mAP50: 0.5097353357108658\n",
            "mAP50-95: 0.2684233878827464\n"
          ]
        }
      ],
      "source": [
        "# 2. Tester le modÃ¨le fine-tunÃ©\n",
        "# Le meilleur modÃ¨le est automatiquement sauvegardÃ© dans runs/detect/trainX/weights/best.pt\n",
        "model_finetuned = YOLO('runs/train23/weights/best.pt')\n",
        "\n",
        "# Validation sur le set de test\n",
        "results = model_finetuned.val(\n",
        "    data=\"varroa-counter-v3-3/data.yaml\",\n",
        "    split='test',  # ou 'val'\n",
        "    imgsz=2048,\n",
        "    conf=0.1,\n",
        "    iou=0.5,\n",
        "    max_det=2000,\n",
        "    save=True\n",
        ")\n",
        "\n",
        "print(f\"Precision: {results.box.p}\")\n",
        "print(f\"Recall: {results.box.r}\")\n",
        "print(f\"mAP50: {results.box.map50}\")\n",
        "print(f\"mAP50-95: {results.box.map}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lE3RPOTFLug",
        "outputId": "d34e66ee-2fd6-4cde-cac4-71e916b91645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.249  Python-3.12.7 torch-2.9.1+cpu CPU (Intel Core(TM) i7-10510U 1.80GHz)\n",
            "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "\u001b[34m\u001b[1mval: \u001b[0mError loading data from None\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:165\u001b[39m, in \u001b[36mBaseDataset.get_img_files\u001b[39m\u001b[34m(self, img_path)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m img_path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img_path, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [img_path]:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     p = \u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# os-agnostic\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m p.is_dir():  \u001b[38;5;66;03m# dir\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\pathlib.py:1162\u001b[39m, in \u001b[36mPath.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1161\u001b[39m     warnings._deprecated(\u001b[33m\"\u001b[39m\u001b[33mpathlib.PurePath(**kwargs)\u001b[39m\u001b[33m\"\u001b[39m, msg, remove=(\u001b[32m3\u001b[39m, \u001b[32m14\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1162\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\pathlib.py:373\u001b[39m, in \u001b[36mPurePath.__init__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    374\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33margument should be a str or an os.PathLike \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    375\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mobject where __fspath__ returns a str, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    376\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    377\u001b[39m paths.append(path)\n",
            "\u001b[31mTypeError\u001b[39m: argument should be a str or an os.PathLike object where __fspath__ returns a str, not 'NoneType'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      4\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33mmodel_mdpi_3291496/weights/best.pt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mVarroa-board-1/data.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvalid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Utiliser le split 'test'\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m6016\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_det\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_width\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_txt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_conf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRÃ©sultats de validation sur Varroa-board-1:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrÃ©cision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults.box.p\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:611\u001b[39m, in \u001b[36mModel.val\u001b[39m\u001b[34m(self, validator, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m args = {**\u001b[38;5;28mself\u001b[39m.overrides, **custom, **kwargs, \u001b[33m\"\u001b[39m\u001b[33mmode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m}  \u001b[38;5;66;03m# highest priority args on the right\u001b[39;00m\n\u001b[32m    610\u001b[39m validator = (validator \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._smart_load(\u001b[33m\"\u001b[39m\u001b[33mvalidator\u001b[39m\u001b[33m\"\u001b[39m))(args=args, _callbacks=\u001b[38;5;28mself\u001b[39m.callbacks)\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[38;5;28mself\u001b[39m.metrics = validator.metrics\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m validator.metrics\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\validator.py:186\u001b[39m, in \u001b[36mBaseValidator.__call__\u001b[39m\u001b[34m(self, trainer, model)\u001b[39m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.rect = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.stride = model.stride  \u001b[38;5;66;03m# used in get_dataloader() for padding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m \u001b[38;5;28mself\u001b[39m.dataloader = \u001b[38;5;28mself\u001b[39m.dataloader \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m model.eval()\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.compile:\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:313\u001b[39m, in \u001b[36mDetectionValidator.get_dataloader\u001b[39m\u001b[34m(self, dataset_path, batch_size)\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_path: \u001b[38;5;28mstr\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m) -> torch.utils.data.DataLoader:\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct and return dataloader.\u001b[39;00m\n\u001b[32m    305\u001b[39m \n\u001b[32m    306\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    311\u001b[39m \u001b[33;03m        (torch.utils.data.DataLoader): DataLoader for validation.\u001b[39;00m\n\u001b[32m    312\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m build_dataloader(\n\u001b[32m    315\u001b[39m         dataset,\n\u001b[32m    316\u001b[39m         batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    321\u001b[39m         pin_memory=\u001b[38;5;28mself\u001b[39m.training,\n\u001b[32m    322\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\models\\yolo\\detect\\val.py:301\u001b[39m, in \u001b[36mDetectionValidator.build_dataset\u001b[39m\u001b[34m(self, img_path, mode, batch)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, img_path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m, batch: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> torch.utils.data.Dataset:\n\u001b[32m    291\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build YOLO Dataset.\u001b[39;00m\n\u001b[32m    292\u001b[39m \n\u001b[32m    293\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    299\u001b[39m \u001b[33;03m        (Dataset): YOLO dataset.\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_yolo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\build.py:236\u001b[39m, in \u001b[36mbuild_yolo_dataset\u001b[39m\u001b[34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\u001b[39;00m\n\u001b[32m    235\u001b[39m dataset = YOLOMultiModalDataset \u001b[38;5;28;01mif\u001b[39;00m multi_modal \u001b[38;5;28;01melse\u001b[39;00m YOLODataset\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[32m    242\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\dataset.py:88\u001b[39m, in \u001b[36mYOLODataset.__init__\u001b[39m\u001b[34m(self, data, task, *args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.data = data\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.use_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_keypoints), \u001b[33m\"\u001b[39m\u001b[33mCan not use both segments and keypoints.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchannels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:117\u001b[39m, in \u001b[36mBaseDataset.__init__\u001b[39m\u001b[34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction, channels)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.channels = channels\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m.cv2_flag = cv2.IMREAD_GRAYSCALE \u001b[38;5;28;01mif\u001b[39;00m channels == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cv2.IMREAD_COLOR\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28mself\u001b[39m.im_files = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_img_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.labels = \u001b[38;5;28mself\u001b[39m.get_labels()\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.update_labels(include_class=classes)  \u001b[38;5;66;03m# single_cls and include_class\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\ultralytics\\data\\base.py:181\u001b[39m, in \u001b[36mBaseDataset.get_img_files\u001b[39m\u001b[34m(self, img_path)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m im_files, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFORMATS_HELP_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mError loading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mHELP_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fraction < \u001b[32m1\u001b[39m:\n\u001b[32m    183\u001b[39m     im_files = im_files[: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(im_files) * \u001b[38;5;28mself\u001b[39m.fraction)]  \u001b[38;5;66;03m# retain a fraction of the dataset\u001b[39;00m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: \u001b[34m\u001b[1mval: \u001b[0mError loading data from None\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance."
          ]
        }
      ],
      "source": [
        "# Test du modÃ¨le avec le dataset Varroa-board-1\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "results = model.val(\n",
        "    data=\"Varroa-board-1/data.yaml\",\n",
        "    split='test',  # Utiliser le split 'test'\n",
        "    imgsz=(6016), max_det=2000, conf=0.1, iou = 0.5,\n",
        "    save=True, show_labels=False, line_width=2, save_txt=True, save_conf=True\n",
        ")\n",
        "\n",
        "print(\"\\nRÃ©sultats de validation sur Varroa-board-1:\")\n",
        "print(f\"PrÃ©cision: {results.box.p}\")\n",
        "print(f\"Recall: {results.box.r}\")\n",
        "print(f\"mAP50: {results.box.map50}\")\n",
        "print(f\"mAP50-95: {results.box.map}\")\n",
        "\n",
        "#RÃ©sultats de validation sur Varroa-board-1:\n",
        "#PrÃ©cision: [    0.80237]\n",
        "#Recall: [    0.42177]\n",
        "#mAP50: 0.584942355545071\n",
        "#mAP50-95: 0.2293648037166341"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOZnqC5FFLuh"
      },
      "outputs": [],
      "source": [
        "# Installation de SAHI pour la dÃ©tection par dÃ©coupage\n",
        "!pip install sahi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4lWQMDoFLuh",
        "outputId": "b7c27439-8bd8-4936-88fc-383267870e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing prediction on 6 slices.\n",
            "Image exportÃ©e dans: results/detection_result.png\n",
            "\n",
            "======================================================================\n",
            "RÃ‰SUMÃ‰ DE LA DÃ‰TECTION\n",
            "======================================================================\n",
            "\n",
            "Nombre total de varroas dÃ©tectÃ©s: 245\n",
            "\n",
            "Statistiques de confiance:\n",
            "  - Confiance moyenne: 0.629\n",
            "  - Confiance min: 0.101\n",
            "  - Confiance max: 0.939\n",
            "\n",
            "Distribution par niveau de confiance:\n",
            "  - 0.1 - 0.3: 61 dÃ©tections\n",
            "  - 0.3 - 0.5: 26 dÃ©tections\n",
            "  - 0.5 - 0.7: 15 dÃ©tections\n",
            "  - 0.7 - 0.9: 98 dÃ©tections\n",
            "  - 0.9 - 1.0: 45 dÃ©tections\n",
            "\n",
            "Statistiques des bounding boxes (en pixels):\n",
            "  - Aire moyenne: 3428.8 pxÂ²\n",
            "  - Aire min: 294.6 pxÂ²\n",
            "  - Aire max: 8172.9 pxÂ²\n",
            "\n",
            "======================================================================\n",
            "DÃ‰TAILS DES 10 PREMIÃˆRES DÃ‰TECTIONS\n",
            "======================================================================\n",
            "\n",
            "DÃ©tection #1:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.939\n",
            "  - Bounding box: x1=2194, y1=3837, x2=2257, y2=3906\n",
            "  - Dimensions: 63x69 pixels\n",
            "  - Centre: (2226, 3871)\n",
            "\n",
            "DÃ©tection #2:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.932\n",
            "  - Bounding box: x1=1761, y1=1243, x2=1821, y2=1311\n",
            "  - Dimensions: 60x68 pixels\n",
            "  - Centre: (1791, 1277)\n",
            "\n",
            "DÃ©tection #3:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.925\n",
            "  - Bounding box: x1=2095, y1=870, x2=2162, y2=930\n",
            "  - Dimensions: 67x60 pixels\n",
            "  - Centre: (2128, 900)\n",
            "\n",
            "DÃ©tection #4:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.925\n",
            "  - Bounding box: x1=2816, y1=2330, x2=2877, y2=2397\n",
            "  - Dimensions: 61x67 pixels\n",
            "  - Centre: (2847, 2363)\n",
            "\n",
            "DÃ©tection #5:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.924\n",
            "  - Bounding box: x1=2766, y1=4686, x2=2832, y2=4747\n",
            "  - Dimensions: 66x60 pixels\n",
            "  - Centre: (2799, 4716)\n",
            "\n",
            "DÃ©tection #6:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.922\n",
            "  - Bounding box: x1=4479, y1=3067, x2=4543, y2=3129\n",
            "  - Dimensions: 64x62 pixels\n",
            "  - Centre: (4511, 3098)\n",
            "\n",
            "DÃ©tection #7:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.921\n",
            "  - Bounding box: x1=1773, y1=2939, x2=1830, y2=3002\n",
            "  - Dimensions: 58x63 pixels\n",
            "  - Centre: (1802, 2970)\n",
            "\n",
            "DÃ©tection #8:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.919\n",
            "  - Bounding box: x1=4247, y1=3177, x2=4310, y2=3245\n",
            "  - Dimensions: 63x68 pixels\n",
            "  - Centre: (4279, 3211)\n",
            "\n",
            "DÃ©tection #9:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.919\n",
            "  - Bounding box: x1=3752, y1=2561, x2=3811, y2=2619\n",
            "  - Dimensions: 59x58 pixels\n",
            "  - Centre: (3782, 2590)\n",
            "\n",
            "DÃ©tection #10:\n",
            "  - Classe: varroa\n",
            "  - Confiance: 0.917\n",
            "  - Bounding box: x1=2083, y1=4202, x2=2148, y2=4260\n",
            "  - Dimensions: 65x58 pixels\n",
            "  - Centre: (2116, 4231)\n"
          ]
        }
      ],
      "source": [
        "# DÃ©tection avec SAHI (Slicing Aided Hyper Inference)\n",
        "from sahi import AutoDetectionModel\n",
        "from sahi.predict import get_sliced_prediction\n",
        "from sahi.utils.cv import read_image\n",
        "import os\n",
        "\n",
        "# Charger le modÃ¨le via SAHI\n",
        "detection_model = AutoDetectionModel.from_pretrained(\n",
        "    model_type=\"yolov8\",  # Compatible avec YOLO11\n",
        "    model_path=\"model_mdpi_3291496/weights/best.pt\",\n",
        "    confidence_threshold=0.1,\n",
        "    device=\"cpu\"\n",
        ")\n",
        "\n",
        "# Image Ã  analyser\n",
        "image_path = 'varroa-counter-v3-3/test/images/IMG_6098_jpg.rf.eb517a2863fbb704a84afce55b287455.jpg'\n",
        "\n",
        "# Effectuer la prÃ©diction par dÃ©coupage\n",
        "result = get_sliced_prediction(\n",
        "    image_path,\n",
        "    detection_model,\n",
        "    slice_height=4096,\n",
        "    slice_width=4096,\n",
        "    overlap_height_ratio=0.2,\n",
        "    overlap_width_ratio=0.2,\n",
        "    postprocess_type=\"NMS\",\n",
        "    postprocess_match_metric=\"IOU\",\n",
        "    postprocess_match_threshold=0.5\n",
        ")\n",
        "\n",
        "# CrÃ©er le dossier de rÃ©sultats\n",
        "output_dir = \"results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Exporter l'image avec SEULEMENT les bounding boxes (sans texte de classe)\n",
        "result.export_visuals(\n",
        "    export_dir=output_dir,\n",
        "    file_name=\"detection_result\",\n",
        "    hide_labels=True,      # Masque le texte de classe\n",
        "    hide_conf=True,        # Masque le score de confiance\n",
        "    rect_th=1              # Ã‰paisseur des rectangles\n",
        ")\n",
        "\n",
        "print(f\"Image exportÃ©e dans: {output_dir}/detection_result.png\")\n",
        "\n",
        "# ============================================================\n",
        "# INFORMATIONS DÃ‰TAILLÃ‰ES SUR LES RÃ‰SULTATS\n",
        "# ============================================================\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"RÃ‰SUMÃ‰ DE LA DÃ‰TECTION\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "object_predictions = result.object_prediction_list\n",
        "print(f\"\\nNombre total de varroas dÃ©tectÃ©s: {len(object_predictions)}\")\n",
        "\n",
        "# Statistiques sur les scores de confiance\n",
        "if object_predictions:\n",
        "    confidences = [pred.score.value for pred in object_predictions]\n",
        "    print(f\"\\nStatistiques de confiance:\")\n",
        "    print(f\"  - Confiance moyenne: {sum(confidences)/len(confidences):.3f}\")\n",
        "    print(f\"  - Confiance min: {min(confidences):.3f}\")\n",
        "    print(f\"  - Confiance max: {max(confidences):.3f}\")\n",
        "\n",
        "    # Distribution par tranches de confiance\n",
        "    print(f\"\\nDistribution par niveau de confiance:\")\n",
        "    ranges = [(0.1, 0.3), (0.3, 0.5), (0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]\n",
        "    for low, high in ranges:\n",
        "        count = sum(1 for c in confidences if low <= c < high)\n",
        "        print(f\"  - {low:.1f} - {high:.1f}: {count} dÃ©tections\")\n",
        "\n",
        "    # Statistiques sur les tailles des bounding boxes\n",
        "    areas = []\n",
        "    for pred in object_predictions:\n",
        "        bbox = pred.bbox\n",
        "        width = bbox.maxx - bbox.minx\n",
        "        height = bbox.maxy - bbox.miny\n",
        "        areas.append(width * height)\n",
        "\n",
        "    print(f\"\\nStatistiques des bounding boxes (en pixels):\")\n",
        "    print(f\"  - Aire moyenne: {sum(areas)/len(areas):.1f} pxÂ²\")\n",
        "    print(f\"  - Aire min: {min(areas):.1f} pxÂ²\")\n",
        "    print(f\"  - Aire max: {max(areas):.1f} pxÂ²\")\n",
        "\n",
        "# Afficher les dÃ©tails des 10 premiÃ¨res dÃ©tections\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"DÃ‰TAILS DES 10 PREMIÃˆRES DÃ‰TECTIONS\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "for i, pred in enumerate(object_predictions[:10]):\n",
        "    bbox = pred.bbox\n",
        "    print(f\"\\nDÃ©tection #{i+1}:\")\n",
        "    print(f\"  - Classe: {pred.category.name}\")\n",
        "    print(f\"  - Confiance: {pred.score.value:.3f}\")\n",
        "    print(f\"  - Bounding box: x1={bbox.minx:.0f}, y1={bbox.miny:.0f}, x2={bbox.maxx:.0f}, y2={bbox.maxy:.0f}\")\n",
        "    print(f\"  - Dimensions: {bbox.maxx - bbox.minx:.0f}x{bbox.maxy - bbox.miny:.0f} pixels\")\n",
        "    print(f\"  - Centre: ({(bbox.minx + bbox.maxx)/2:.0f}, {(bbox.miny + bbox.maxy)/2:.0f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AUV_dWwFLuh"
      },
      "outputs": [],
      "source": [
        "# Exporter les rÃ©sultats dÃ©taillÃ©s en CSV\n",
        "import csv\n",
        "\n",
        "csv_path = os.path.join(output_dir, \"detections.csv\")\n",
        "\n",
        "with open(csv_path, 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['id', 'classe', 'confiance', 'x1', 'y1', 'x2', 'y2', 'largeur', 'hauteur', 'aire', 'centre_x', 'centre_y'])\n",
        "\n",
        "    for i, pred in enumerate(object_predictions):\n",
        "        bbox = pred.bbox\n",
        "        width = bbox.maxx - bbox.minx\n",
        "        height = bbox.maxy - bbox.miny\n",
        "        area = width * height\n",
        "        center_x = (bbox.minx + bbox.maxx) / 2\n",
        "        center_y = (bbox.miny + bbox.maxy) / 2\n",
        "\n",
        "        writer.writerow([\n",
        "            i + 1,\n",
        "            pred.category.name,\n",
        "            f\"{pred.score.value:.4f}\",\n",
        "            f\"{bbox.minx:.1f}\",\n",
        "            f\"{bbox.miny:.1f}\",\n",
        "            f\"{bbox.maxx:.1f}\",\n",
        "            f\"{bbox.maxy:.1f}\",\n",
        "            f\"{width:.1f}\",\n",
        "            f\"{height:.1f}\",\n",
        "            f\"{area:.1f}\",\n",
        "            f\"{center_x:.1f}\",\n",
        "            f\"{center_y:.1f}\"\n",
        "        ])\n",
        "\n",
        "print(f\"RÃ©sultats exportÃ©s en CSV: {csv_path}\")\n",
        "print(f\"\\nLe fichier contient {len(object_predictions)} dÃ©tections avec:\")\n",
        "print(\"  - ID unique\")\n",
        "print(\"  - Classe dÃ©tectÃ©e\")\n",
        "print(\"  - Score de confiance\")\n",
        "print(\"  - CoordonnÃ©es du bounding box (x1, y1, x2, y2)\")\n",
        "print(\"  - Dimensions (largeur, hauteur)\")\n",
        "print(\"  - Aire en pixelsÂ²\")\n",
        "print(\"  - CoordonnÃ©es du centre\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGmzjkyQFLuh",
        "outputId": "21c6e3aa-cef8-4f48-9271-e43a55acd495"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33mVarroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Charger l'image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m width, height = img.size\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTaille originale de l\u001b[39m\u001b[33m'\u001b[39m\u001b[33mimage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32md:\\dev\\python\\Python312\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'"
          ]
        }
      ],
      "source": [
        "# Test avec dÃ©coupage d'image en 4 parties\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Charger le modÃ¨le\n",
        "model = YOLO('model_mdpi_3291496/weights/best.pt')\n",
        "\n",
        "# Chemin de l'image Ã  tester\n",
        "image_path = 'Varroa-board-1/test/images/IMG_0226_jpg.rf.c97161f83bb98300231bd6318d7dee3b.jpg'\n",
        "\n",
        "# Charger l'image\n",
        "img = Image.open(image_path)\n",
        "width, height = img.size\n",
        "print(f\"Taille originale de l'image: {width}x{height}\")\n",
        "\n",
        "# CrÃ©er un dossier pour les images dÃ©coupÃ©es\n",
        "output_dir = 'temp_tiles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# DÃ©couper l'image en 4 parties (2x2)\n",
        "half_width = width // 2\n",
        "half_height = height // 2\n",
        "\n",
        "tiles = []\n",
        "positions = [\n",
        "    (0, 0, half_width, half_height, \"top_left\"),\n",
        "    (half_width, 0, width, half_height, \"top_right\"),\n",
        "    (0, half_height, half_width, height, \"bottom_left\"),\n",
        "    (half_width, half_height, width, height, \"bottom_right\")\n",
        "]\n",
        "\n",
        "# DÃ©couper et sauvegarder chaque partie\n",
        "for i, (x1, y1, x2, y2, name) in enumerate(positions):\n",
        "    tile = img.crop((x1, y1, x2, y2))\n",
        "    tile_path = os.path.join(output_dir, f'tile_{i+1}_{name}.jpg')\n",
        "    tile.save(tile_path)\n",
        "    tiles.append((tile_path, name, x1, y1))\n",
        "    print(f\"Partie {i+1} ({name}): {tile.size[0]}x{tile.size[1]} sauvegardÃ©e\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DÃ‰TECTION SUR CHAQUE PARTIE\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Tester le modÃ¨le sur chaque partie\n",
        "total_detections = 0\n",
        "all_results = []\n",
        "\n",
        "for tile_path, name, offset_x, offset_y in tiles:\n",
        "    print(f\"\\n--- DÃ©tection sur {name} ---\")\n",
        "    results = model.predict(\n",
        "        source=tile_path,\n",
        "   imgsz=(6016), max_det=2000, conf=0.1, iou = 0.5,\n",
        "    save=True, show_labels=False, line_width=2, save_txt=True, save_conf=True\n",
        "    )\n",
        "\n",
        "    detections = len(results[0].boxes)\n",
        "    total_detections += detections\n",
        "    all_results.append((name, detections, results[0]))\n",
        "\n",
        "    print(f\"Varroas dÃ©tectÃ©s: {detections}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"RÃ‰SUMÃ‰ DES DÃ‰TECTIONS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"\\nNombre total de varroas dÃ©tectÃ©s: {total_detections}\")\n",
        "print(\"\\nDÃ©tail par partie:\")\n",
        "for name, count, _ in all_results:\n",
        "    print(f\"  - {name:15s}: {count:3d} varroas\")\n",
        "\n",
        "print(f\"\\nImages dÃ©coupÃ©es sauvegardÃ©es dans: {output_dir}/\")\n",
        "print(f\"RÃ©sultats de dÃ©tection sauvegardÃ©s dans: runs/detect/predict*/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTv47LxBFLuh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def create_filtered_dataset(source_dir, dest_dir, prefix=\"IMG_6\"):\n",
        "    \"\"\"\n",
        "    CrÃ©e une copie d'un dataset YOLO en ne gardant que les images\n",
        "    dont le nom commence par le prÃ©fixe spÃ©cifiÃ©.\n",
        "\n",
        "    Args:\n",
        "        source_dir: Chemin du dataset source (ex: \"varroa-counter-v3-3\")\n",
        "        dest_dir: Chemin du dataset destination\n",
        "        prefix: PrÃ©fixe des images Ã  conserver (ex: \"IMG_6\")\n",
        "    \"\"\"\n",
        "    source_dir = Path(source_dir)\n",
        "    dest_dir = Path(dest_dir)\n",
        "\n",
        "    # Copier data.yaml et fichiers README\n",
        "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for f in source_dir.glob(\"*.yaml\"):\n",
        "        shutil.copy2(f, dest_dir / f.name)\n",
        "    for f in source_dir.glob(\"*.txt\"):\n",
        "        shutil.copy2(f, dest_dir / f.name)\n",
        "\n",
        "    total_copied = 0\n",
        "\n",
        "    # Parcourir les splits (train, valid, test)\n",
        "    for split_dir in source_dir.iterdir():\n",
        "        if not split_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        images_dir = split_dir / \"images\"\n",
        "        labels_dir = split_dir / \"labels\"\n",
        "\n",
        "        if not images_dir.exists():\n",
        "            continue\n",
        "\n",
        "        # CrÃ©er les dossiers de destination\n",
        "        dest_images = dest_dir / split_dir.name / \"images\"\n",
        "        dest_labels = dest_dir / split_dir.name / \"labels\"\n",
        "        dest_images.mkdir(parents=True, exist_ok=True)\n",
        "        dest_labels.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Copier les images correspondant au prÃ©fixe et leurs labels\n",
        "        for img_file in images_dir.iterdir():\n",
        "            if img_file.name.startswith(prefix):\n",
        "                shutil.copy2(img_file, dest_images / img_file.name)\n",
        "\n",
        "                # Copier le label correspondant (.txt)\n",
        "                label_file = labels_dir / (img_file.stem + \".txt\")\n",
        "                if label_file.exists():\n",
        "                    shutil.copy2(label_file, dest_labels / label_file.name)\n",
        "\n",
        "                total_copied += 1\n",
        "\n",
        "    print(f\"Dataset filtrÃ© crÃ©Ã© dans: {dest_dir}\")\n",
        "    print(f\"Images copiÃ©es: {total_copied}\")\n",
        "\n",
        "    # Lister le contenu\n",
        "    for split_dir in sorted(dest_dir.iterdir()):\n",
        "        if split_dir.is_dir():\n",
        "            imgs = list((split_dir / \"images\").glob(\"*\"))\n",
        "            print(f\"  {split_dir.name}: {len(imgs)} images\")\n",
        "\n",
        "\n",
        "# CrÃ©er le dataset filtrÃ© avec uniquement les images IMG_6xxx\n",
        "create_filtered_dataset(\n",
        "    source_dir=\"varroa-counter-v3-3\",\n",
        "    dest_dir=\"varroa-counter-v3-3-IMG6\",\n",
        "    prefix=\"IMG_6\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}